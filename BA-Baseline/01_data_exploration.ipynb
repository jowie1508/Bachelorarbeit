{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Processing the WPUQ data: Load heat pump and weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Exploration of the heat pump load data: Data availability, analysis of missing intervals, time series plots, aggregated load and correlation, influence of day type, operation mode, nominal capacity of heat pumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Exploration of the weather data: statistical distribution, time series plots, correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Exploration of additional information: building information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep this notebook clearly readable, some functions are outsourced in utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing WPUQ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_to_pandas(hdf_dataset):\n",
    "    '''\n",
    "    input:  hdf table\n",
    "    output: pandas DataFrame\n",
    "    '''\n",
    "    column_type_dict = {x:str(y[0]) for x,y in hdf_dataset.dtype.fields.items()}\n",
    "    column_list, list_of_rows = [], []\n",
    "    for index in column_type_dict:\n",
    "        column_list.append(index)\n",
    "    for line in range(0, hdf_dataset.size):\n",
    "        list_of_rows.append(np.asarray(hdf_dataset[line]).tolist())\n",
    "\n",
    "    return pd.DataFrame(data=list_of_rows, columns=column_list)\n",
    "\n",
    "def first_n_digits(num, n):\n",
    "    return num // 10 ** (int(math.log(num, 10)) - n + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load heat pump data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the load pump data for 36 houses in hdf5 format, each year stored in a seperate file and convert the data format to a python dictionary containing the load data of each house over the available time span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path_WPUQ = \"data/WPUQ/heatpumps\"\n",
    "weather_path_WPUQ = \"data/WPUQ/weather\"\n",
    "path_concat = \"data/raw\"\n",
    "path_cleaned = \"data/cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "file = h5py.File(f'{load_path_WPUQ}/2018_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2018 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2019\n",
    "file = h5py.File(f'{load_path_WPUQ}/2019_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2019 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2020\n",
    "file = h5py.File(f'{load_path_WPUQ}/2020_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2020 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# concat \n",
    "df_dict = {}\n",
    "\n",
    "for key_house in df_dict_2020:\n",
    "    df_dict[key_house] = pd.concat([df_dict_2018[key_house], df_dict_2019[key_house], df_dict_2020[key_house]])\n",
    "\n",
    "for key_house in df_dict:\n",
    "    if len(df_dict[key_house]) != 105216:\n",
    "        print(\"issue with \" + str(key_house))\n",
    "\n",
    "print(\"data for {} houses\".format(len(df_dict)))\n",
    "\n",
    "with open(f'{path_concat}/data_heatpump.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(f'{weather_path_WPUQ}/weather/2018_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2018 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2018[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2018[key][\"index\"] = weather_dict_2018[key][\"index\"].apply(lambda x: first_n_digits(x, 10))\n",
    "\n",
    "file = h5py.File(f'{weather_path_WPUQ}/weather/2019_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2019 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2019[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2019[key][\"index\"] = weather_dict_2019[key][\"index\"].apply(lambda x: first_n_digits(x, 10))\n",
    "\n",
    "file = h5py.File(f'{weather_path_WPUQ}/weather/2020_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2020 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2020[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2020[key][\"index\"] = weather_dict_2020[key][\"index\"].apply(lambda x: first_n_digits(x, 10))\n",
    "\n",
    "weather_dict = {}\n",
    "\n",
    "for parameter in weather_dict_2018:\n",
    "    weather_dict[parameter] = pd.concat([weather_dict_2018[parameter],weather_dict_2019[parameter],weather_dict_2020[parameter]])\n",
    "\n",
    "with open(f'{path_concat}/data_weather.pkl', 'wb') as f:\n",
    "    pickle.dump(weather_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge weather data - create consistent index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "ref_index = load_dict['SFH10']['index']\n",
    "\n",
    "df_list = []\n",
    "for df_type in weather_dict:\n",
    "    df_ref = ref_index.to_frame().set_index('index')\n",
    "    df_ref[df_type] = np.nan\n",
    "    df_temp = weather_dict[df_type]\n",
    "    for index in ref_index:\n",
    "        sub_df = df_temp[(df_temp['index'] >= index) & (df_temp['index'] <= index+900)]\n",
    "        if sub_df.empty:\n",
    "            #take previous value\n",
    "            df_ref.loc[index][df_type] = df_ref.loc[index-900][df_type]\n",
    "        else:\n",
    "            #take mean value\n",
    "            df_ref.loc[index][df_type] = sub_df.iloc[:,1].mean()\n",
    "    df_list.append(df_ref)\n",
    "weather_data = pd.concat(df_list, axis=1)\n",
    "with open(f'{path_concat}/data_weather_merged.pkl', 'wb') as f:\n",
    "    pickle.dump(weather_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration of Heat Pump Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_concat}/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_data_availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dataframe containing binary information about data availability for each timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_nan)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set time zone to germany \n",
    "import locale\n",
    "locale.setlocale(locale.LC_TIME, 'de_DE')\n",
    "\n",
    "plot_data_availability(df_result[df_result.index > 1525125600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improve data quality by changing start time and selecting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index from start of data availability of SFH37\n",
    "df_reduced = df_result[df_result.index >= 1528965000] \n",
    "plot_data_availability(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85\n",
    "column_list=[]\n",
    "missing_list=[]\n",
    "incomplete_list = []\n",
    "complete_list = []\n",
    "for column in df_reduced.columns:\n",
    "    percentage = df_reduced[column].sum()/len(df_reduced)\n",
    "    if percentage > threshold:\n",
    "        if percentage != 1:\n",
    "            incomplete_list.append(column)\n",
    "        if percentage ==1: \n",
    "            complete_list.append(column)\n",
    "        column_list.append(column)\n",
    "    else:\n",
    "        missing_list.append(column)\n",
    "\n",
    "print('reduced to {} datasets'.format(len(column_list)))\n",
    "\n",
    "plot_data_availability(df_reduced[column_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analysis of missing intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_intervals(df, column):\n",
    "    \"\"\"\n",
    "    Identifies contiguous intervals of missing data (represented by zeros) in a given DataFrame column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame containing the data.\n",
    "    - column (str): The name of the column to analyze for missing data intervals.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: Each tuple represents a missing data interval with the start and end indices.\n",
    "    \"\"\"\n",
    "    # Mark groups of contiguous values (different from the previous value)\n",
    "    df['group'] = (df[column] != df[column].shift()).cumsum()\n",
    "\n",
    "    # Identify groups where the column value is 0 (missing data)\n",
    "    zero_groups = df[df[column] == 0].groupby('group')\n",
    "\n",
    "    # Initialize an empty list to store the start and end indices of missing intervals\n",
    "    result = []\n",
    "\n",
    "    # Iterate through each group of zeros and record the start and end index\n",
    "    for _, group in zero_groups:\n",
    "        start_index = group.index[0]\n",
    "        end_index = group.index[-1]\n",
    "        result.append((start_index, end_index))\n",
    "\n",
    "    # Remove the 'group' column to clean up the DataFrame\n",
    "    df.drop(columns='group', inplace=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_interval_length(interval):\n",
    "    return interval[1] - interval[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_intervalls = {}\n",
    "\n",
    "df_result = df_result[df_result.index > 1528965000]\n",
    "for column in df_result.columns:\n",
    "    if column in incomplete_list:\n",
    "        df = df_result[column].to_frame()\n",
    "        intervalls = get_missing_intervals(df, column)\n",
    "        dict_intervalls[column] = intervalls\n",
    "        #print(column + \": \" + str(intervalls))\n",
    "\n",
    "#with open('Data/missing_intervalls_dict.pkl', 'wb') as f:\n",
    "#    pickle.dump(dict_intervalls, f)\n",
    "\n",
    "dict_intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"complete list 100%: \" + str(complete_list))\n",
    "print(\"incomplete list >85%: \" + str(incomplete_list))\n",
    "print(\"insufficient list <85%: \" + str(missing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "nb = 0\n",
    "month_dict={}\n",
    "\n",
    "for key in dict_intervalls:\n",
    "    tuple_list = []\n",
    "    for intervalls in dict_intervalls[key]:\n",
    "        start = pd.to_datetime(intervalls[0], unit='s').month\n",
    "        end = pd.to_datetime(intervalls[1], unit='s').month\n",
    "        secs = intervalls[1]-intervalls[0]\n",
    "        sum += secs\n",
    "        nb +=1\n",
    "        tuple_list.append((start, end))\n",
    "    month_dict[key] = [month for month in range(start, end, 1)]\n",
    "\n",
    "print(\"Mittlere Anzahl an fehlenden Tagen: \" + str(sum/((60*60*24)*nb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of missing intervals per month and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping sensor identifiers to the months with missing data\n",
    "dict_months = {\n",
    "    'SFH10': [11, 12],\n",
    "    'SFH11': [5, 6, 7, 8, 9],\n",
    "    'SFH20': [6, 7, 8, 9, 10, 11],\n",
    "    'SFH21': [6, 7, 8],\n",
    "    'SFH23': [7, 8, 9],\n",
    "    'SFH38': [6, 7, 8],\n",
    "    'SFH39': [6, 7, 8],\n",
    "    'SFH5': [8, 9],\n",
    "    'SFH7': [9, 10, 11],\n",
    "}\n",
    "\n",
    "# Extract all unique months from the dictionary values to ensure coverage\n",
    "unique_months = sorted(set(month for months in dict_months.values() for month in months))\n",
    "\n",
    "# List of month names for x-axis labels\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Creating the stacked bar chart\n",
    "fig = go.Figure()\n",
    "for key in dict_months:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=key,\n",
    "        x=[month_names[month - 1] for month in unique_months],  # Convert month numbers to names\n",
    "        y=[1 if month in dict_months[key] else 0 for month in unique_months]  # Presence of the month in the data\n",
    "    ))\n",
    "\n",
    "# Updating layout properties using update methods\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Number of Missing Records per Month',\n",
    "    title_x=0.5,\n",
    "    title_y=0.85,\n",
    "    yaxis_title='Number of Missing Months',\n",
    "    legend_title='Dataset',\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    title='Month',\n",
    "    tickvals=[month_names[month - 1] for month in unique_months],  # Set custom tick values\n",
    "    ticktext=[month_names[month - 1] for month in unique_months]  # Set custom tick labels\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Time series plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_consumption, plot_resampled_consumption, plot_yearly_resampled_consumption, plot_consumption_type_histo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = load_dict[\"SFH3\"]\n",
    "plot_consumption(df_3, 'SFH3')\n",
    "plot_resampled_consumption(df_3, 'SFH3')\n",
    "plot_yearly_resampled_consumption(df_3, 'SFH3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Aggregated Load - Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START = 1525270500\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START = 1525270500\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']\n",
    "\n",
    "# exemplary index\n",
    "df_3 = load_dict[\"SFH3\"]\n",
    "df_3 = df_3[df_3['index']>INDEX_START]\n",
    "df_3.set_index('index', inplace=True)\n",
    "df_3.index = pd.to_datetime(df_3.index, unit='s')\n",
    "\n",
    "# aggregate load of all datasets\n",
    "df_summe = pd.DataFrame(index=df_3.index, columns=COLUMNS)\n",
    "for key in load_dict:\n",
    "    df_house = load_dict[key].copy()\n",
    "    df_house['index'] = pd.to_datetime(df_house['index'], unit='s')\n",
    "    df_house.set_index('index', inplace=True)\n",
    "    df_house = df_house[df_house.index > pd.to_datetime(INDEX_START, unit='s')]\n",
    "    df_house = df_house[COLUMNS]\n",
    "    df_summe = df_summe.fillna(0) + df_house.fillna(0)\n",
    "\n",
    "# calculate mean of cos(phi)  \n",
    "df_summe['PF_TOT'] = df_summe['PF_TOT']/len(load_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation matrix\n",
    "correlation_matrix = df_summe.corr()\n",
    "ylabels = ['Wirkleistung P', 'Scheinleistung S', 'Blindleistung Q', 'Leistungsfaktor cos(φ)']\n",
    "xlabels = ['P', 'S', 'Q', 'cos(φ)']\n",
    "\n",
    "heatmap = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix',fontsize=14)\n",
    "heatmap.set_yticklabels(ylabels, rotation=0, fontsize=14)\n",
    "heatmap.set_xticklabels(xlabels, rotation=45, fontsize=14)  # Rotation für bessere Lesbarkeit\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Influence of day type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the DataFrame index to datetime, assuming the index represents Unix timestamps\n",
    "df_summe.index = pd.to_datetime(df_summe.index, unit='s')\n",
    "\n",
    "# Separate the DataFrame into weekdays and weekends based on the day of the week\n",
    "df_weekdays = df_summe[df_summe.index.dayofweek < 5]  # Monday=0, Sunday=6\n",
    "df_weekend = df_summe[df_summe.index.dayofweek >= 5]\n",
    "\n",
    "# Calculate the average values for weekdays and weekends\n",
    "weekdays_avg = df_weekdays[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "weekend_avg = df_weekend[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "\n",
    "# Create a new DataFrame with the average values\n",
    "df_avg = pd.DataFrame({'Weekdays': weekdays_avg, 'Weekend': weekend_avg})\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add bars for each dataset\n",
    "for i, col in enumerate(df_avg.columns):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_avg.index,  # Column names are shown on the y-axis\n",
    "        x=df_avg[col],  # Average values are shown on the x-axis\n",
    "        name=col,  # Legend name\n",
    "        orientation='h',  # Horizontal bars\n",
    "        text=df_avg[col].round(2),  # Display rounded average values inside the bars\n",
    "        textposition='inside'\n",
    "        # marker_color=colors[i]  # Color of the bars, assuming 'colors' is a predefined list\n",
    "    ))\n",
    "\n",
    "# Update the layout for a grouped bar chart\n",
    "fig.update_layout(\n",
    "    barmode='group',  # Group the bars\n",
    "    title='Average Values for Weekdays and Weekends',\n",
    "    title_x=0.5,\n",
    "    xaxis_title='Average Values',\n",
    "    yaxis_title='',\n",
    "    legend_title='Type of Day',\n",
    "    bargap=0.2,  # Space between the bar groups\n",
    "    template='simple_white'\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Operation modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > P < 100W: Standby\n",
    "- > 100W < P < 4kW: compressor mode\n",
    "- > P > 4kW: heating rod mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_operation_mode(x):\n",
    "    if x < 100:\n",
    "        return 1\n",
    "    elif (x > 100) & (x < 4000):\n",
    "        return 2\n",
    "    elif x >=4000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_operation_mode)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot histogram for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2018 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2018.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2018]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2018.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2018.columns:\n",
    "    df_consumptions_2018[column] = df_consumptions_2018[column]/1000\n",
    "df_consumptions_2018.head()\n",
    "plot_consumption_type_histo(df_consumptions_2018, 2018)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2019 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2019.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2019]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2019.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2019.columns:\n",
    "    df_consumptions_2019[column] = df_consumptions_2019[column]/1000\n",
    "df_consumptions_2019.head()\n",
    "plot_consumption_type_histo(df_consumptions_2019,2019)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2020 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2020.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2020]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2020.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2020.columns:\n",
    "    df_consumptions_2020[column] = df_consumptions_2020[column]/1000\n",
    "df_consumptions_2020.head()\n",
    "plot_consumption_type_histo(df_consumptions_2020, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Calculation of heat pump nominal power "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import data_utils\n",
    "import config \n",
    "from utils.plot_utils import plot_consumption_with_band, plot_quantile_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_utils.data_loader(config.columns)\n",
    "\n",
    "for index in data.index.unique():\n",
    "    sub_df = data[data.index==index]\n",
    "    if sub_df[\"P_TOT\"].max() < 7900:\n",
    "        continue\n",
    "    elif len(sub_df[sub_df[\"P_TOT\"]>7900]) < 200:\n",
    "        continue\n",
    "    else:\n",
    "        sub_df = sub_df.set_index(\"index\")\n",
    "        sub_df.index = pd.to_datetime(sub_df.index, unit=\"s\")\n",
    "        plot_consumption_with_band(sub_df, '2019-01-01', '2021-01-01', index)\n",
    "\n",
    "plot_quantile_comparison(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration of Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_concat}/data_weather.pkl', 'rb') as f:\n",
    "    weather_dict = pickle.load(f)\n",
    "with open(f'{path_concat}/data_weather_merged.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WEATHER_APPARENT_TEMPERATURE_TOTAL</th>\n",
       "      <th>WEATHER_ATMOSPHERIC_PRESSURE_TOTAL</th>\n",
       "      <th>WEATHER_PRECIPITATION_RATE_TOTAL</th>\n",
       "      <th>WEATHER_PROBABILITY_OF_PRECIPITATION_TOTAL</th>\n",
       "      <th>WEATHER_RELATIVE_HUMIDITY_TOTAL</th>\n",
       "      <th>WEATHER_SOLAR_IRRADIANCE_GLOBAL</th>\n",
       "      <th>WEATHER_TEMPERATURE_TOTAL</th>\n",
       "      <th>WEATHER_WIND_DIRECTION_TOTAL</th>\n",
       "      <th>WEATHER_WIND_GUST_SPEED_TOTAL</th>\n",
       "      <th>WEATHER_WIND_SPEED_TOTAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.0</td>\n",
       "      <td>977.599976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-12.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.099998</td>\n",
       "      <td>1047.699951</td>\n",
       "      <td>38.550001</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>37.400002</td>\n",
       "      <td>360.0</td>\n",
       "      <td>34.166668</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.919311</td>\n",
       "      <td>1015.48095</td>\n",
       "      <td>0.085169</td>\n",
       "      <td>34.733337</td>\n",
       "      <td>72.6693</td>\n",
       "      <td>139.729787</td>\n",
       "      <td>11.020408</td>\n",
       "      <td>196.629154</td>\n",
       "      <td>7.416688</td>\n",
       "      <td>2.985826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>10.2</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>210.0</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>2.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing values</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               WEATHER_APPARENT_TEMPERATURE_TOTAL  \\\n",
       "min                                         -18.0   \n",
       "max                                     48.099998   \n",
       "mean                                     9.919311   \n",
       "median                                       10.2   \n",
       "missing values                                  0   \n",
       "\n",
       "               WEATHER_ATMOSPHERIC_PRESSURE_TOTAL  \\\n",
       "min                                    977.599976   \n",
       "max                                   1047.699951   \n",
       "mean                                   1015.48095   \n",
       "median                                     1016.0   \n",
       "missing values                                  0   \n",
       "\n",
       "               WEATHER_PRECIPITATION_RATE_TOTAL  \\\n",
       "min                                         0.0   \n",
       "max                                   38.550001   \n",
       "mean                                   0.085169   \n",
       "median                                      0.0   \n",
       "missing values                                0   \n",
       "\n",
       "               WEATHER_PROBABILITY_OF_PRECIPITATION_TOTAL  \\\n",
       "min                                                   5.0   \n",
       "max                                                 100.0   \n",
       "mean                                            34.733337   \n",
       "median                                               35.0   \n",
       "missing values                                          0   \n",
       "\n",
       "               WEATHER_RELATIVE_HUMIDITY_TOTAL  \\\n",
       "min                                       19.0   \n",
       "max                                      100.0   \n",
       "mean                                   72.6693   \n",
       "median                                    75.0   \n",
       "missing values                               0   \n",
       "\n",
       "               WEATHER_SOLAR_IRRADIANCE_GLOBAL WEATHER_TEMPERATURE_TOTAL  \\\n",
       "min                                        0.0                     -12.6   \n",
       "max                                      975.0                 37.400002   \n",
       "mean                                139.729787                 11.020408   \n",
       "median                                     5.0                      10.2   \n",
       "missing values                               0                         0   \n",
       "\n",
       "               WEATHER_WIND_DIRECTION_TOTAL WEATHER_WIND_GUST_SPEED_TOTAL  \\\n",
       "min                                     0.0                      0.277778   \n",
       "max                                   360.0                     34.166668   \n",
       "mean                             196.629154                      7.416688   \n",
       "median                                210.0                      6.666667   \n",
       "missing values                            0                             0   \n",
       "\n",
       "               WEATHER_WIND_SPEED_TOTAL  \n",
       "min                                 0.1  \n",
       "max                                14.1  \n",
       "mean                           2.985826  \n",
       "median                            2.675  \n",
       "missing values                        0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(columns=weather_data.columns, index=['min', 'max', 'mean', 'median', 'missing values'])\n",
    "for column in weather_data.columns:\n",
    "    df_analysis.loc['min'][column] = weather_data[column].min()\n",
    "    df_analysis.loc['max'][column] = weather_data[column].max()\n",
    "    df_analysis.loc['mean'][column] = weather_data[column].mean()\n",
    "    df_analysis.loc['median'][column] = weather_data[column].median()\n",
    "    df_analysis.loc['missing values'][column] = len(weather_data) - weather_data[column].value_counts().sum()\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plots = weather_data.copy()\n",
    "data_plots.index = pd.to_datetime(data_plots.index, unit=\"s\")\n",
    "\n",
    "# Create a subplot grid with 5 rows and 2 columns\n",
    "fig = make_subplots(rows=5, cols=2, subplot_titles=data_plots.columns, vertical_spacing=0.05)\n",
    "\n",
    "# Map DataFrame columns to subplots\n",
    "row_col_pairs = [(row, col) for row in range(1, 6) for col in range(1, 3)]\n",
    "for (column, (row, col)) in zip(data_plots.columns, row_col_pairs):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=data_plots.index, y=data_plots[column], name=column),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Customize the layout to match the Plotly White style\n",
    "fig.update_layout(\n",
    "    title_text='Verlauf verschiedener Spalten über die Zeit',\n",
    "    title_x=0.5,\n",
    "    showlegend=False,\n",
    "    template='plotly_white',\n",
    "    height=2000  # Adjust height to provide enough space for all subplots\n",
    ")\n",
    "\n",
    "# Customize x-axis ticks for all subplots\n",
    "for axis in fig.layout:\n",
    "    if axis.startswith('xaxis'):\n",
    "        fig.layout[axis].tickangle = 45\n",
    "\n",
    "# Display the plots\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot time series for temperature and wind speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = weather_data.copy()\n",
    "df.index = pd.to_datetime(df.index, unit=\"s\")\n",
    "\n",
    "# Create a subplot layout with 1 row and 2 columns\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=('Messwerte Temperatur', 'Messwerte Windgeschwindigkeit'))\n",
    "\n",
    "# Add the first column as a scatter plot to the subplot\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['WEATHER_TEMPERATURE_TOTAL'], name='Temperatur'), row=1, col=1)\n",
    "\n",
    "# Add the second column as a scatter plot to the subplot\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df['WEATHER_WIND_SPEED_TOTAL'], name='Windgeschwindigkeit'), row=1, col=2)\n",
    "\n",
    "# Axis labels for the first subplot\n",
    "fig.update_xaxes(title_text='Zeit', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Temperatur in °C', row=1, col=1)\n",
    "\n",
    "# Axis labels for the second subplot\n",
    "fig.update_xaxes(title_text='Zeit', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Windgeschwindigkeit in m/s', row=1, col=2)\n",
    "\n",
    "# Update the layout to display the plots side by side\n",
    "fig.update_layout(title_text='Verlauf der Wettermessdaten', showlegend=False, title_x=0.5, template='plotly_white', width=900)\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dict = {\n",
    "    'WEATHER_APPARENT_TEMPERATURE_TOTAL':           'Scheintemperatur',\n",
    "    'WEATHER_ATMOSPHERIC_PRESSURE_TOTAL':           'Luftdruck',\n",
    "    'WEATHER_PRECIPITATION_RATE_TOTAL':             'Niederschlag',\n",
    "    'WEATHER_PROBABILITY_OF_PRECIPITATION_TOTAL':   'Niederschlagswahrscheinlichkeit',\n",
    "    'WEATHER_RELATIVE_HUMIDITY_TOTAL':              'Relative Luftfeuchtigkeit',\n",
    "    'WEATHER_SOLAR_IRRADIANCE_GLOBAL':              'Sonneneinstrahlung',\n",
    "    'WEATHER_TEMPERATURE_TOTAL':                    'Temperatur',\n",
    "    'WEATHER_WIND_DIRECTION_TOTAL':                 'Windrichtung',\n",
    "    'WEATHER_WIND_GUST_SPEED_TOTAL':                'Windböenstärke',\n",
    "    'WEATHER_WIND_SPEED_TOTAL':                     'Windgeschwindigkeit'\n",
    "}\n",
    "\n",
    "\n",
    "correlation_matrix = weather_data.rename(columns=columns_dict).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix')\n",
    "#plt.xlabel('Variablen')\n",
    "#plt.ylabel('Variablen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Entfernen der Scheintemperatur sowie der Windböenstärke, da diese von der Absoluttemperatur sowie der Windgeschwindigkeit bereits gut erfasst werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_weather_data = weather_data.drop(columns=['WEATHER_APPARENT_TEMPERATURE_TOTAL', 'WEATHER_WIND_GUST_SPEED_TOTAL'])\n",
    "with open(f'{path_cleaned}/data_weather_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(reduced_weather_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploration of Additional Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Building number</th>\n",
       "      <th>Building area</th>\n",
       "      <th>Number of inhabitants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Building number  Building area  Number of inhabitants\n",
       "0                3          140.0                      2\n",
       "1                4          160.0                      2\n",
       "2                5          160.0                      3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = pd.read_excel('data/raw/Gebaeudeinformationen.xlsx', header=0)\n",
    "info.at[27, \"Building area\"] = info[info[\"Number of inhabitants\"]==3].dropna()[\"Building area\"].mean()\n",
    "info.to_excel(\"data/cleaned/Gebaeudeinformationen_cleaned.xlsx\")\n",
    "info.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
