{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load Data Quality: Linear Regression, Reduction of data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Merge Load and Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Prepare Data for Training: Standardization, Creating Sequences, Trainings/Val/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep this notebook clearly readable, some functions are outsourced in utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pickle\n",
    "import re\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path_WPUQ = \"data/WPUQ/heatpumps\"\n",
    "weather_path_WPUQ = \"data/WPUQ/weather\"\n",
    "path_concat = \"data/raw\"\n",
    "path_cleaned = \"data/cleaned\"\n",
    "\n",
    "INDEX_START = 1528965000\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_concat}/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "with open(f'{path_cleaned}/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)\n",
    "\n",
    "with open('data/missing_intervalls_dict.pkl', 'rb') as f:\n",
    "    missing_intervals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='plots/data_availability>85.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set index to start index\n",
    "for key in load_dict:\n",
    "    df_house = load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index > INDEX_START]\n",
    "    df_house = df_house[COLUMNS]\n",
    "\n",
    "    for column in df_house.columns:\n",
    "        if not df_house[df_house[column]<0].empty:\n",
    "            df_house.loc[df_house[column] < 0, column] = 0.01\n",
    "\n",
    "    load_dict[key] = df_house   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filling Missing Values using Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incomplete time series\n",
    "list_complete = ['SFH12', 'SFH14', 'SFH16', 'SFH18', 'SFH19', 'SFH22', 'SFH27', 'SFH28', 'SFH29', \n",
    "                 'SFH3', 'SFH30', 'SFH32', 'SFH34', 'SFH36', 'SFH4', 'SFH9', 'SFH26', 'SFH33']\n",
    "list_incomplete = ['SFH5', 'SFH7', 'SFH10', 'SFH11', 'SFH20', 'SFH21', 'SFH23', 'SFH38', 'SFH39']\n",
    "list_incomlete_unique = ['SFH5', 'SFH7', 'SFH10', 'SFH11', 'SFH21', 'SFH38', 'SFH39']\n",
    "list_incomplete_double = ['SFH20', 'SFH23']\n",
    "list_v1 = list_complete + list_incomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(load_dict['SFH3'].index[0], unit='s')\n",
    "end = pd.to_datetime(load_dict['SFH3'].index[-1], unit='s')\n",
    "\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import train_and_predict\n",
    "from utils.plot_utils import plot_metrics_lr, plot_consumption_filled\n",
    "dict_result, df_metrics = train_and_predict(load_dict, weather_data, missing_intervals, list_incomplete, include_time_features=False)\n",
    "plot_metrics_lr(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list_incomplete: #[\"SFH11\"]:\n",
    "    dict_result[key] = dict_result[key].clip(lower=0)\n",
    "    plot_consumption_filled(dict_result[key], ['P_TOT', 'PF_TOT'], key, missing_intervals[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add already complete time series\n",
    "for key in list_complete:\n",
    "    dict_result[key] = load_dict[key]\n",
    "# save to file\n",
    "with open(f'{path_cleaned}/data_heatpump_cleaned_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Further reduction of data sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(1542512700, unit='s')\n",
    "end = pd.to_datetime(load_dict['SFH3'].index[-1], unit='s')\n",
    "\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{path_concat}/data_heatpump.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "load_dict = {}\n",
    "\n",
    "# set index to start index\n",
    "for key in list_v1:\n",
    "    if key in ['SFH10', 'SFH11', 'SFH23']:\n",
    "        #drop datasets\n",
    "        continue\n",
    "    else:\n",
    "        df_house = data[key].set_index('index')\n",
    "        # start index after missing values for SFH7\n",
    "        df_house = df_house[df_house.index > missing_intervals['SFH7'][0][1]]\n",
    "        df_house = df_house[COLUMNS]\n",
    "        df_house = df_house.clip(lower=0)\n",
    "\n",
    "        load_dict[key] = df_house   \n",
    "\n",
    "with open(f'{path_cleaned}/data_heatpump_cleaned_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(load_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge Load and Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "# Load cleaned heat pump data\n",
    "with open('data/cleaned/data_heatpump_cleaned_v1.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "# Load weather data\n",
    "with open('Data/cleaned/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)\n",
    "\n",
    "# Load building information and set the index\n",
    "building_info = pd.read_excel(\"data/cleaned/Gebaeudeinformationen_cleaned.xlsx\", index_col=0)\n",
    "building_info.set_index(\"Building number\", inplace=True)\n",
    "\n",
    "load_dict_sorted = {}\n",
    "\n",
    "# Add building information and merge with weather data\n",
    "for house in sorted(load_dict, key=lambda x: int(re.findall(r'\\d+', x)[0])):\n",
    "    id = int(re.findall(r'\\d+', house)[0])\n",
    "\n",
    "    # Add building area, number of inhabitants, and building id to each house's data\n",
    "    load_dict[house][\"area\"] = building_info.loc[id][\"Building area\"]\n",
    "    load_dict[house][\"inhabitants\"] = building_info.loc[id][\"Number of inhabitants\"]\n",
    "    load_dict[house][\"building\"] = id\n",
    "    \n",
    "    # Filter weather data and merge with house data\n",
    "    weather_data_filtered = weather_data[weather_data.index >= 1528965900]\n",
    "    load_dict[house] = pd.concat([load_dict[house], weather_data_filtered], axis=1)\n",
    "    load_dict[house].reset_index(inplace=True)\n",
    "    load_dict[house] = load_dict[house][load_dict[house][\"index\"] > 1546298100]\n",
    "\n",
    "    load_dict_sorted[house] = load_dict[house][config.columns]\n",
    "\n",
    "# Concatenate data for all houses\n",
    "data = pd.concat(load_dict_sorted)\n",
    "\n",
    "# Reset index and set the new index to the house id, dropping the old index column\n",
    "data = data.reset_index().set_index(\"level_0\").drop(columns=\"level_1\")\n",
    "\n",
    "with open(f'{path_cleaned}/merged_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    \n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_scaling \n",
    "\n",
    "print(\"scaled_data.shape: \" + str(scaled_data.shape))\n",
    "plot_scaling(df_scaled, data, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Creating Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import create_daily_sequences\n",
    "from utils.plot_utils import plot_sequences\n",
    "\n",
    "# Erstellen von Sequenzen\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for building in df_scaled[\"building\"].unique():\n",
    "    df_building = df_scaled[df_scaled[\"building\"]==building]\n",
    "    X_building, y_building = create_daily_sequences(df_scaled[df_scaled[\"building\"]==building], config.SEQUENZE_LENGTH, config.PREDICTION_LENGTH, num_target_var=1)\n",
    "    # Hinzufügen der Sequenzen zur Gesamtliste\n",
    "    all_X.append(X_building)\n",
    "    all_y.append(y_building)\n",
    "\n",
    "X = np.concatenate(all_X, axis=0)\n",
    "y = np.concatenate(all_y, axis=0)\n",
    "\n",
    "print(\"Dimensionen X: \" + str(X.shape))\n",
    "print(\"Dimensionen y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Split Trainings, Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import train_test_val_data\n",
    "from utils.plot_utils import plot_with_classification\n",
    "\n",
    "len_dataset = len(data.index.unique())\n",
    "num_target_variables = 1\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_data(df_scaled, len_dataset, num_target_variables)\n",
    "\n",
    "print(\"Dimensionen X_train: \" + str(X_train.shape))\n",
    "print(\"Dimensionen X_val: \" + str(X_val.shape))\n",
    "print(\"Dimensionen X_test: \" + str(X_test.shape))\n",
    "print(\"Dimensionen y_train: \" + str(y_train.shape))\n",
    "print(\"Dimensionen y_val: \" + str(y_val.shape))\n",
    "print(\"Dimensionen y_test: \" + str(y_test.shape))\n",
    "\n",
    "plot_with_classification(data[data.index==\"SFH23\"], train_split=0.5,  val_split=0.95, combine=True, title=\"SFH23\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
