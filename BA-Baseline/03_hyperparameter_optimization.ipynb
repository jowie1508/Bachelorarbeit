{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters: Influence and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Influence of various Hyperparameters on Trainings process: input/output sequence length, learning rate, regularization, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hyperparameteroptimization using RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 1 Optimization P-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 2 Optimization PF-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep this notebook clearly readable, some functions are outsourced in utils/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from kerastuner import HyperModel, RandomSearch, HyperParameters\n",
    "from keras.optimizers import Adam, Adagrad, Adadelta, SGD\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import data_loader\n",
    "from utils.utils import train_test_val_data\n",
    "import config\n",
    "\n",
    "# load data\n",
    "data = data_loader(config.columns_P)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_data(df_scaled, \n",
    "                                                                     len(data.index.unique()), \n",
    "                                                                     1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Influence of various Hyperparameters on Trainings process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_validation_losses_and_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Influence input sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_0 = 64\n",
    "units_outputs = y_train.shape[1]\n",
    "batch_size = 32\n",
    "\n",
    "curves_sequences = []\n",
    "train_durations = []\n",
    "\n",
    "for sequence_length in [1,3,6]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_data(df_scaled, len_dataset, num_target_variables, sequence_length=sequence_length)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, activation='linear'))\n",
    "\n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer='adam',                   # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=30,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    curves_sequences.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations.append(training_duration)\n",
    "\n",
    "with open('data/hyper_analysis/21_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(curves_sequences, f)\n",
    "\n",
    "with open('data/hyper_analysis/21_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyper_analysis/21_curves.pkl', 'rb') as f:\n",
    "    curves_sequences = pickle.load(f)\n",
    "\n",
    "with open('data/hyper_analysis/21_durations.pkl', 'rb') as f:\n",
    "    train_durations = pickle.load(f)\n",
    "    \n",
    "plot_validation_losses_and_durations(curves_sequences, [\"1 Tag\", \"3 Tage\", \"6 Tage\"], train_durations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Influence prediction length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_0 = 64\n",
    "units_outputs = y_train.shape[1]\n",
    "batch_size = 32\n",
    "\n",
    "curves_prediction_length = []\n",
    "train_durations_2 = []\n",
    "\n",
    "for prediction_length in [(1/24),(6/24),(12/24),1]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_val_data(df_scaled, len_dataset, num_target_variables, prediction_length=prediction_length)\n",
    "\n",
    "    units_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, activation='linear'))\n",
    "\n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer='adam',                   # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    curves_prediction_length.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations_2.append(training_duration)\n",
    "    \n",
    "with open('data/hyper_analysis/22_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(curves_prediction_length, f)\n",
    "\n",
    "with open('data/hyper_analysis/22_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyper_analysis/22_curves.pkl', 'rb') as f:\n",
    "    curves_prediction_length = pickle.load(f)\n",
    "\n",
    "with open('data/hyper_analysis/22_durations.pkl', 'rb') as f:\n",
    "    train_durations_2 = pickle.load(f)\n",
    "\n",
    "plot_validation_losses_and_durations(curves_prediction_length, [\"1 Stunde\",\"6 Stunden\", \"12 Stunden\", \"24 Stunden\"], train_durations_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Influence learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_curves = []\n",
    "train_durations_3 = []\n",
    "\n",
    "for learning_rate in [0.01, 0.001, 0.0001, 0.00001]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_val_data(df_scaled, len_dataset, num_target_variables)\n",
    "    units_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, activation='linear'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer=optimizer,                # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    learning_rate_curves.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations_3.append(training_duration)\n",
    "    \n",
    "with open('data/hyper_analysis/23_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(learning_rate_curves, f)\n",
    "\n",
    "with open('data/hyper_analysis/23_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyper_analysis/23_curves.pkl', 'rb') as f:\n",
    "    learning_rate_curves = pickle.load(f)\n",
    "\n",
    "with open('data/hyper_analysis/23_durations.pkl', 'rb') as f:\n",
    "    train_durations_3 = pickle.load(f)\n",
    "\n",
    "plot_validation_losses_and_durations(learning_rate_curves, [\"0.1\",\"0.01\", \"0.001\", \"0.0001\"], train_durations_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_2_curves = []\n",
    "train_durations_5 = []\n",
    "\n",
    "for learning_rate in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_val_data(df_scaled, len_dataset, num_target_variables)\n",
    "    units_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, activation='linear'))\n",
    "\n",
    "    optimizer = Adagrad(learning_rate=learning_rate)\n",
    "    \n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer=optimizer,                # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    learning_rate_2_curves.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations_5.append(training_duration)\n",
    "\n",
    "with open('data/hyper_analysis/232_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(learning_rate_2_curves, f)\n",
    "\n",
    "with open('data/hyper_analysis/232_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations_5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyper_analysis/232_curves.pkl', 'rb') as f:\n",
    "    learning_rate_2_curves= pickle.load(f)\n",
    "\n",
    "with open('data/hyper_analysis/232_durations.pkl', 'rb') as f:\n",
    "    train_durations_5 = pickle.load(f)\n",
    "\n",
    "plot_validation_losses_and_durations(learning_rate_2_curves, [\"0.1\", \"0.01\", \"0.001\", \"0.0001\", \"0.00001\"], train_durations_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Einfluss Optimierer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_curves = []\n",
    "train_durations_4 = []\n",
    "\n",
    "for optimizer in [Adadelta(learning_rate=0.01), SGD(learning_rate=0.01), Adagrad(learning_rate=0.01), Adam(learning_rate=0.0047)]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_val_data(df_scaled, len_dataset, num_target_variables)\n",
    "    units_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, activation='linear'))\n",
    "    \n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer=optimizer,                   # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    optimizer_curves.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations_4.append(training_duration)\n",
    "\n",
    "optimizer_curves_2 = optimizer_curves\n",
    "optimizer_curves_2[3] = learning_rate_curves[1]\n",
    "train_durations_4_2 = train_durations_4\n",
    "train_durations_4_2[3] = train_durations_3[1]\n",
    "\n",
    "with open('data/hyper_analysis/231_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(optimizer_curves_2, f)\n",
    "\n",
    "with open('data/hyper_analysis/231_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations_4_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/hyper_analysis/231_curves.pkl', 'rb') as f:\n",
    "    optimizer_curves_2 = pickle.load(f)\n",
    "\n",
    "with open('data/hyper_analysis/231_durations.pkl', 'rb') as f:\n",
    "    train_durations_4_2 = pickle.load(f)\n",
    "\n",
    "plot_validation_losses_and_durations(optimizer_curves_2, [\"Adadelta\", \"SGD\", \"Adagrad\", \"Adam\"], train_durations_4_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Influence Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_0 = 64\n",
    "units_outputs = y_train.shape[1]\n",
    "batch_size = 32\n",
    "\n",
    "reg_curves = []\n",
    "train_durations_5 = []\n",
    "\n",
    "for l1l2 in [(0.001, 0.001), (0.0001,0.0001), (0.00001, 0.00001)]:\n",
    "    len_dataset = len(data.index.unique())\n",
    "    num_target_variables = 1\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = utils.train_test_val_data(df_scaled, len_dataset, num_target_variables)\n",
    "    units_outputs = y_train.shape[1]\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Eingabeschicht\n",
    "    model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "                return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                            # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                            #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                            #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                            #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                            # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                            #            ansonsten: return_sequences=False.\n",
    "                                            # \n",
    "                input_shape=(\n",
    "                    X_train.shape[1],           # Sequenzlänge\n",
    "                    X_train.shape[2]            # Anzahl der Features\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ausgabeschicht\n",
    "    model.add(Dense(units_outputs, \n",
    "                    activation='linear',\n",
    "                    kernel_regularizer=l1_l2(l1=l1l2[0], l2=l1l2[1])\n",
    "                    ))\n",
    "\n",
    "    \n",
    "    # Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "    model.compile(\n",
    "        optimizer='adam',                # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                            # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                            # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                            #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                            #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                            #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                            #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                            #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                            # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "        loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "        metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "        )\n",
    "    \n",
    "    # Trainieren des Modells\n",
    "    history = model.fit(\n",
    "        X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "        epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                            #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "        batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                            #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "        validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "        callbacks=[PlotLossesKeras()],\n",
    "        use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "        workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "        verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    "    )\n",
    "\n",
    "    reg_curves.append(history)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time  # Dauer in Sekunden\n",
    "\n",
    "    train_durations_5.append(training_duration)\n",
    "\n",
    "with open('data/hyper_analysis/25_curves.pkl', 'wb') as f:\n",
    "    pickle.dump(reg_curves, f)\n",
    "\n",
    "with open('data/hyper_analysis/25_durations.pkl', 'wb') as f:\n",
    "    pickle.dump(train_durations_5, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_losses_and_durations(reg_curves, [\"(10e-3, 10e-3)\", \"(10e-4, 10e-4)\", \"(10e-5, 10e-5)\"], train_durations_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameteroptimierung using Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 P-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUHyperModel(HyperModel):\n",
    "    '''\n",
    "    Hypermodel used for Hyperparameteroptimization\n",
    "    '''\n",
    "    def __init__(self, input_shape, output_units):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(GRU(\n",
    "            units=hp.get('units_0'),\n",
    "            return_sequences=hp.get('num_gru_layers') > 1,\n",
    "            input_shape=self.input_shape\n",
    "        ))\n",
    "        model.add(Dropout(hp.get('dropout_rate_0')))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        for i in range(1, hp.get('num_gru_layers')):\n",
    "            model.add(GRU(\n",
    "                units=hp.get(f'units_{i}'),\n",
    "                return_sequences=i < hp.get('num_gru_layers') - 1\n",
    "            ))\n",
    "            model.add(Dropout(hp.get(f'dropout_rate_{i}')))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(self.output_units, \n",
    "                        activation='linear',\n",
    "                        kernel_regularizer=l1_l2(l1=hp.get('l1'),\n",
    "                                           l2=hp.get('l2'))), \n",
    "                  )\n",
    "        model.compile(\n",
    "        optimizer=Adam(hp.get('learning_rate')),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "        return model\n",
    "    \n",
    "# definition of Hyperparameters\n",
    "hp = HyperParameters()\n",
    "hp.Int('num_gru_layers', 1, 5, default=2)\n",
    "hp.Int('units_0', 32, 256, step=32)\n",
    "hp.Float('dropout_rate_0', 0, 0.5, step=0.1)\n",
    "for i in range(1, 5):  \n",
    "    hp.Int(f'units_{i}', 32, 256, step=32, default=32)\n",
    "    hp.Float(f'dropout_rate_{i}', 0, 0.5, step=0.1, default=0.1)\n",
    "hp.Choice('batch_size', values=[32, 64, 128])\n",
    "hp.Float('learning_rate', min_value=1e-4, \n",
    "        max_value=1e-2, sampling='LOG')\n",
    "hp.Float('l1', min_value=1e-5, \n",
    "        max_value=1e-4, sampling='LOG')\n",
    "hp.Float('l2', min_value=1e-5, \n",
    "        max_value=1e-4, sampling='LOG')\n",
    "\n",
    "# input and output shape\n",
    "input_shape = (None, 13) \n",
    "output_units = 96 \n",
    "\n",
    "# initialize tuner\n",
    "tuner = RandomSearch(\n",
    "    GRUHyperModel(input_shape, output_units),\n",
    "    hyperparameters=hp,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='data/tuner',\n",
    "    project_name='gru_hyperparam_P'\n",
    ")\n",
    "\n",
    "# start sweep\n",
    "tuner.search(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=hp.get('batch_size')  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import get_optimization_results\n",
    "\n",
    "results = get_optimization_results(tuner, 100)\n",
    "results = results[results['val_loss'] == results['val_loss'].min()]\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 PF-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_loader(config.columns_PF)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_test_val_data(df_scaled, \n",
    "                                                                     len(data.index.unique()), \n",
    "                                                                     1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUHyperModel(HyperModel):\n",
    "    '''\n",
    "    Hypermodel used for Hyperparameteroptimization\n",
    "    '''\n",
    "    def __init__(self, input_shape, output_units):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Input layer\n",
    "        model.add(GRU(\n",
    "            units=hp.get('units_0'),\n",
    "            return_sequences=hp.get('num_gru_layers') > 1,\n",
    "            input_shape=self.input_shape\n",
    "        ))\n",
    "        model.add(Dropout(hp.get('dropout_rate_0')))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        for i in range(1, hp.get('num_gru_layers')):\n",
    "            model.add(GRU(\n",
    "                units=hp.get(f'units_{i}'),\n",
    "                return_sequences=i < hp.get('num_gru_layers') - 1\n",
    "            ))\n",
    "            model.add(Dropout(hp.get(f'dropout_rate_{i}')))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(self.output_units, activation='linear'))\n",
    "        model.compile(\n",
    "        optimizer=Adam(hp.get('learning_rate')),\n",
    "        loss='mean_squared_error'\n",
    "    )\n",
    "        return model\n",
    "    \n",
    "# definition of Hyperparameters\n",
    "hp = HyperParameters()\n",
    "hp.Int('num_gru_layers', 1, 5, default=2)\n",
    "hp.Int('units_0', 32, 256, step=32)\n",
    "hp.Float('dropout_rate_0', 0, 0.5, step=0.1)\n",
    "for i in range(1, 5):  \n",
    "    hp.Int(f'units_{i}', 32, 256, step=32, default=32)\n",
    "    hp.Float(f'dropout_rate_{i}', 0, 0.5, step=0.1, default=0.1)\n",
    "hp.Choice('batch_size', values=[32, 64, 128])\n",
    "hp.Float('learning_rate', min_value=1e-4, \n",
    "        max_value=1e-2, sampling='LOG')\n",
    "\n",
    "# input and output shape\n",
    "input_shape = (None, 13) \n",
    "output_units = 96 \n",
    "\n",
    "# initialize tuner\n",
    "tuner = RandomSearch(\n",
    "    GRUHyperModel(input_shape, output_units),\n",
    "    hyperparameters=hp,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='data/tuner',\n",
    "    project_name='gru_hyperparam_cosphi'\n",
    ")\n",
    "\n",
    "# start sweep\n",
    "tuner.search(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=hp.get('batch_size')  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_optimization_results(tuner, 100)\n",
    "results = results[results['val_loss'] == results['val_loss'].min()]\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
