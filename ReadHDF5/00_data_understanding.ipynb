{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains code for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2. Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDF5 file, convert to pandas format, concat data for 2018-2020, prepare for use, analysis of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains the code to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read in the load pump data for 36 houses in hdf5 format, each year stored in a seperate file and convert the data format to a python dictionary containing the load data of each house over the available time span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analysis of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data availabilty\n",
    "- visualizations of seperate load profiles\n",
    "- visualizations of aggregated load profiles\n",
    "- influence of week day type\n",
    "- operating mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Further project information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globale Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START = 1525270500\n",
    "INDEX_START_2 = 1528965000\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_to_pandas(hdf_dataset):\n",
    "    column_type_dict = {x:str(y[0]) for x,y in hdf_dataset.dtype.fields.items()}\n",
    "    column_list = []\n",
    "    for index in column_type_dict:\n",
    "        column_list.append(index)\n",
    "    list_of_rows = []\n",
    "    for line in range(0, hdf_dataset.size):\n",
    "        list_of_rows.append(np.asarray(hdf_dataset[line]).tolist())\n",
    "    return pd.DataFrame(data=list_of_rows, columns=column_list)\n",
    "\n",
    "def first_n_digits(num, n):\n",
    "    return num // 10 ** (int(math.log(num, 10)) - n + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in hdf5 data and convert to pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2018_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2018 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2019\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2019_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2019 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2020\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2020_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2020 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# concat \n",
    "df_dict = {}\n",
    "\n",
    "for key_house in df_dict_2020:\n",
    "    df_dict[key_house] = pd.concat([df_dict_2018[key_house], df_dict_2019[key_house], df_dict_2020[key_house]])\n",
    "\n",
    "for key_house in df_dict:\n",
    "    if len(df_dict[key_house]) != 105216:\n",
    "        print(\"issue with \" + str(key_house))\n",
    "\n",
    "print(\"data for {} houses\".format(len(df_dict)))\n",
    "\n",
    "with open('Data/heatpump/data_heatpump.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dataframe, which indicates missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_nan)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_availability_histogramm(df):\n",
    "    # Datenverfügbarkeit berechnen\n",
    "    data_availability = df.mean(axis=0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Farben festlegen\n",
    "    colors = ['#ff9999' if value < 1 else '#66b266' for value in data_availability]\n",
    "\n",
    "    # Horizontales Balkendiagramm zeichnen\n",
    "    data_availability.plot(kind='barh', color=colors, ax=ax)\n",
    "\n",
    "    # Achsentitel und Plot-Titel hinzufügen\n",
    "    ax.set_title('Datenverfügbarkeit')\n",
    "    ax.set_ylabel('Objekte')\n",
    "    ax.set_xlabel('Verfügbarkeit in %')\n",
    "    ax.set_xlim(0, 1)  # x-Achse auf den Bereich 0 bis 1 setzen\n",
    "\n",
    "    # Anzeigen\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data_availability(data):\n",
    "    df = data.copy()\n",
    "    # Datenkonvertierung: Unix-Timestamp zu Datum\n",
    "    df.index = pd.to_datetime(df.index, unit='s')\n",
    "    print(df.index[0])\n",
    "\n",
    "    # Reihenfolge der Spalten nach den Zahlen in den Objektbezeichnungen sortieren\n",
    "    sorted_columns = sorted(df.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "\n",
    "    # Prozentsätze für jede Spalte berechnen\n",
    "    percentages = (df.sum() / len(df) * 100).round(2)\n",
    "    percentages = percentages[sorted_columns]\n",
    "\n",
    "    # Plot-Einstellungen, Verkleinerung der Figur\n",
    "    fig, ax = plt.subplots(figsize=(7, 8))  # Kleinere Figurgröße\n",
    "\n",
    "    # Durch jede sortierte Spalte iterieren und Datenverfügbarkeit zeichnen\n",
    "    for i, column in enumerate(sorted_columns):\n",
    "        # Datenverfügbarkeit\n",
    "        ax.fill_between(df.index, i, i + 1, where=(df[column] == 1), color='#66D37A', step='mid')\n",
    "        # Fehlende Daten\n",
    "        ax.fill_between(df.index, i, i + 1, where=(df[column] == 0), color='#FF5252', step='mid')\n",
    "        # Prozentsatz neben jedem Balken hinzufügen (mit zusätzlichem Leerzeichen und Abstand nach rechts)\n",
    "        ax.text(df.index[-1] + pd.Timedelta(days=1), i + 0.5, f\" {percentages[column]}%\", verticalalignment='center', horizontalalignment='left')\n",
    "\n",
    "    # Einstellungen\n",
    "    # Anpassung der x-Limits, um den Platz für die Prozentsätze zu berücksichtigen\n",
    "    ax.set_xlim([df.index.min(), df.index.max() + pd.Timedelta(days=1)])  # Reduzierter Platz für Prozentsätze\n",
    "    ax.set_ylim([0, len(sorted_columns)])\n",
    "    ax.set_yticks(np.arange(len(sorted_columns)) + 0.5)\n",
    "    ax.set_yticklabels(sorted_columns)\n",
    "    ax.set_title(\"Datenverfügbarkeit\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Haushalt\")\n",
    "\n",
    "    # Farblegende hinzufügen\n",
    "    ax.legend(handles=[plt.Line2D([0], [0], color='#66D37A', label='Vorhanden'),\n",
    "                    plt.Line2D([0], [0], color='#FF5252', label='Fehlt')], loc='upper right')\n",
    "\n",
    "    # Erweiterte Datumseinstellungen für die X-Achse mit 3-monatigem Intervall\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability_histogramm(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability(df_result[df_result.index > 1525125600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result[df_result.index > 1525125600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop functions to analize intervalls with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_intervalls(df, column):\n",
    "    # Gruppen von zusammenhängenden Nullen identifizieren\n",
    "    df['group'] = (df[column] != df[column].shift()).cumsum()\n",
    "    zero_groups = df[df[column] == 0].groupby('group')\n",
    "    # Start- und Endindizes von zusammenhängenden Nullen ausgeben\n",
    "    result = []\n",
    "    for name, group in zero_groups:\n",
    "        start_index = group.index[0]\n",
    "        end_index = group.index[-1]\n",
    "        result.append((start_index, end_index))\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_missing_intervalls_length(tuple):\n",
    "    length = tuple[1] - tuple[0]\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = df_result['SFH10'].to_frame()\n",
    "df_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_intervalls(df_10, 'SFH10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce dataframe to common start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_result[df_result.index >= 1525270500]\n",
    "\n",
    "for row,value in test.iterrows():\n",
    "    if test.loc[row].sum() > 30:\n",
    "        print(\">30: \" + str(row))\n",
    "        start_index = row\n",
    "        break\n",
    "\n",
    "df_final = df_result[df_result.index >= start_index]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability_histogramm(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_result[df_result.index >= 1528965000] # index from start of data availability of SFH37\n",
    "plot_data_availability(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85\n",
    "column_list=[]\n",
    "missing_list=[]\n",
    "incomplete_list = []\n",
    "complete_list = []\n",
    "for column in df_reduced.columns:\n",
    "    percentage = df_reduced[column].sum()/len(df_reduced)\n",
    "    if percentage > threshold:\n",
    "        if percentage != 1:\n",
    "            incomplete_list.append(column)\n",
    "        if percentage ==1: \n",
    "            complete_list.append(column)\n",
    "        column_list.append(column)\n",
    "    else:\n",
    "        missing_list.append(column)\n",
    "\n",
    "print('reduced to {} datasets'.format(len(column_list)))\n",
    "\n",
    "plot_data_availability(df_reduced[column_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analysis of missing intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_intervalls = {}\n",
    "\n",
    "df_result = df_result[df_result.index > INDEX_START_2]\n",
    "for column in df_result.columns:\n",
    "    if column in incomplete_list:\n",
    "        df = df_result[column].to_frame()\n",
    "        intervalls = get_missing_intervalls(df, column)\n",
    "        dict_intervalls[column] = intervalls\n",
    "        #print(column + \": \" + str(intervalls))\n",
    "\n",
    "#with open('Data/missing_intervalls_dict.pkl', 'wb') as f:\n",
    "#    pickle.dump(dict_intervalls, f)\n",
    "\n",
    "dict_intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"complete list 100%: \" + str(complete_list))\n",
    "print(\"incomplete list >85%: \" + str(incomplete_list))\n",
    "print(\"insufficient list <85%: \" + str(missing_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "nb = 0\n",
    "month_dict={}\n",
    "for key in dict_intervalls:\n",
    "    #print(key)\n",
    "    tuple_list = []\n",
    "    for intervalls in dict_intervalls[key]:\n",
    "        start = pd.to_datetime(intervalls[0], unit='s').month\n",
    "        end = pd.to_datetime(intervalls[1], unit='s').month\n",
    "        secs = intervalls[1]-intervalls[0]\n",
    "        sum += secs\n",
    "        nb +=1\n",
    "        #print([month for month in range(start, end+1, 1)])\n",
    "        tuple_list.append((start, end))\n",
    "        \n",
    "    month_dict[key] = [month for month in range(start, end, 1)]\n",
    "print(\"Mittlere Anzahl an fehlenden Tagen: \" + str(sum/((60*60*24)*nb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_months = {\n",
    "    'SFH10': [11,12],\n",
    "    'SFH11': [5,6,7,8,9],\n",
    "    'SFH20': [6,7,8,9,10,11],\n",
    "    'SFH21': [6,7,8],\n",
    "    'SFH23': [7,8,9],\n",
    "    'SFH38': [6,7,8],\n",
    "    'SFH39': [6,7,8],\n",
    "    'SFH5': [8,9],\n",
    "    'SFH7': [9,10,11],\n",
    "}\n",
    "\n",
    "# Extrahieren aller einzigartigen Monate\n",
    "unique_months = sorted(set(month for months in dict_months.values() for month in months))\n",
    "\n",
    "# Monatsnamen für die Achsenbeschriftung\n",
    "month_names = ['Jan', 'Feb', 'Mär', 'Apr', 'Mai', 'Jun', 'Jul', 'Aug', 'Sep', 'Okt', 'Nov', 'Dez']\n",
    "\n",
    "# Erstellen des gestapelten Balkendiagramms\n",
    "fig = go.Figure()\n",
    "for key in dict_months:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=key,\n",
    "        x=[month_names[month-1] for month in unique_months],\n",
    "        y=[1 if month in dict_months[key] else 0 for month in unique_months]\n",
    "    ))\n",
    "\n",
    "# Verwenden von Update-Methoden statt dict()\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    title='Fehlende Monate pro Datensatz',\n",
    "    title_x=0.5,\n",
    "    yaxis_title='Anzahl der fehlenden Monate',\n",
    "    legend_title='SFH',\n",
    ")\n",
    "fig.update_xaxes(\n",
    "    title='Monat',\n",
    "    tickvals=[month_names[month-1] for month in unique_months],\n",
    "    ticktext=[month_names[month-1] for month in unique_months]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of all available data for whole timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_plot(data, name):\n",
    "    df = data.reset_index()\n",
    "    # Erstelle eine Figur mit sekundärer Y-Achse\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Kurven für P_TOT, Q_TOT, und S_TOT zur linken Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['P_TOT'], name='P_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['Q_TOT'], name='Q_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['S_TOT'], name='S_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    # Füge die Kurve für PF_TOT zur rechten Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['PF_TOT'], name='PF_TOT', line=dict(dash='dot')),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    # Benenne die Achsen\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(\n",
    "        title_text='Zeitliche Darstellung der Werte - {}'.format(name),\n",
    "        xaxis=dict(\n",
    "            tickmode='auto',\n",
    "            nticks=20,\n",
    "            ticks='outside',\n",
    "            tickson='boundaries',\n",
    "            ticklen=20\n",
    "        )\n",
    "    )\n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "#create_consumption_plot(df_3, 'SFH3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resampled_consumption_plot(data, name):\n",
    "    # Stelle sicher, dass 'index' in datetime umgewandelt wird und als Index gesetzt wird\n",
    "    df = data.copy()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "    df.set_index('index', inplace=True)\n",
    "\n",
    "    # Resample der Daten auf 24-Stunden-Intervalle und berechne den Durchschnitt\n",
    "    df_resampled = df.resample('24H').mean().reset_index()\n",
    "\n",
    "    # Erstelle eine Figur mit sekundärer Y-Achse\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Kurven für P_TOT, Q_TOT, und S_TOT zur linken Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['P_TOT'], name='P_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['Q_TOT'], name='Q_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['S_TOT'], name='S_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    # Füge die Kurve für PF_TOT zur rechten Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['PF_TOT'], name='PF_TOT', line=dict(dash='dot')),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # Benenne die Achsen\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(\n",
    "        title_text='Zeitliche Darstellung der Werte mit 24-Stunden-Intervallen - {}'.format(name),\n",
    "        xaxis=dict(\n",
    "            tickmode='auto',\n",
    "            nticks=20,\n",
    "            ticks='outside',\n",
    "            tickson='boundaries',\n",
    "            ticklen=20\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Zeige die Figur an\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(string):\n",
    "    # Finde alle Zahlen im String und verbinde sie\n",
    "    number = int(re.search(r'\\d+', string).group())\n",
    "    return number\n",
    "\n",
    "list_strings = list(load_dict.keys())\n",
    "sorted_keys = sorted(list_strings, key=sort_key)\n",
    "\n",
    "for key in sorted_keys[2:4]: #remove [0:5] to see all\n",
    "    sub_df = load_dict[key]\n",
    "    sub_df = sub_df[sub_df['index']>INDEX_START]\n",
    "    sub_df.set_index('index', inplace=True)\n",
    "    sub_df.index = pd.to_datetime(sub_df.index, unit='s')\n",
    "    sub_df.reset_index(inplace=True)\n",
    "    create_resampled_consumption_plot(sub_df, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year(df_year, year, name):\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['PF_TOT'], name='PF_TOT', line=dict(dash='dot')), secondary_y=True)\n",
    "    \n",
    "    # Update the layout for the subplot\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(height=600, width=1200, title_text=f'Daten für das Jahr {year} - {name}')\n",
    "    \n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "def create_plot_per_year(data, name, years=[2018,2019,2020]):\n",
    "    df = data.copy()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "\n",
    "    # Filtere die Daten nach Jahr\n",
    "    for year in years:\n",
    "        df_year = df[df['index'].dt.year == year]\n",
    "        plot_year(df_year, year, name)\n",
    "\n",
    "#create_plot_per_year(df_3, 'SFH3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung der Plots mit 24-stündiger Auflösung\n",
    "def create_resampled_plot_per_year(data, year, name):\n",
    "    df = data.reset_index()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "    df.set_index('index', inplace=True)\n",
    "    # Resample der Daten auf 24-Stunden-Intervalle und berechne den Durchschnitt\n",
    "    df_resampled = df.resample('24H').mean()\n",
    "    \n",
    "    # Filtere nur das gewünschte Jahr\n",
    "    df_year = df_resampled[df_resampled.index.year == year]\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Daten zur Figur hinzu\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['PF_TOT'], name='PF_TOT', line=dict(dash='dot')), secondary_y=True)\n",
    "\n",
    "    # Aktualisiere die Layout-Einstellungen\n",
    "    fig.update_layout(title_text=f\"Durchschnittliche Werte in 24-Stunden-Intervallen für das Jahr {year} - {name}\",\n",
    "                      height=600, width=1400)\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "\n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "# Erzeuge die Plots für jedes Jahr\n",
    "# for year in range(2018,2021,1):\n",
    "    #create_resampled_plot_per_year(df_3, year, 'SFH3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_moving_average(data, bez):\n",
    "    # Angenommen, df ist Ihr DataFrame, der bereits geladen wurde und das Datum als Index hat.\n",
    "    df=data.resample('24H').mean()\n",
    "\n",
    "    # Erstellen Sie eine Subplot-Figur mit zwei Y-Achsen\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Hinzufügen der Linien für 'P_TOT', 'Q_TOT', und 'S_TOT' auf der linken Y-Achse\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "\n",
    "    # Hinzufügen der Linie für 'PF_TOT' auf der rechten Y-Achse\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['PF_TOT'], name='PF_TOT', marker_color='red'), secondary_y=True)\n",
    "\n",
    "    # Berechnen des gleitenden Mittelwerts für 'P_TOT' und Hinzufügen als Linie\n",
    "    df['P_TOT_SMA'] = df['P_TOT'].rolling(window=3).mean()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['P_TOT_SMA'], name='P_TOT SMA', line=dict(color='firebrick', width=2, dash='dash')), secondary_y=False)\n",
    "\n",
    "    # Update der Layouts für Achsen und Titel\n",
    "    fig.update_layout(title_text=\"24h resampled Lastverlauf - {}\".format(bez), title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"\", dtick='M1')\n",
    "    fig.update_yaxes(title_text=\"P_TOT, Q_TOT, S_TOT [W, var, VA]\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"PF_TOT\", secondary_y=True)\n",
    "\n",
    "    # Anzeigen der Figur\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Aggregated load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = load_dict[\"SFH3\"]\n",
    "df_3 = df_3[df_3['index']>INDEX_START]\n",
    "df_3.set_index('index', inplace=True)\n",
    "df_3.index = pd.to_datetime(df_3.index, unit='s')\n",
    "\n",
    "df_summe = pd.DataFrame(index=df_3.index, columns=COLUMNS)\n",
    "for key in load_dict:\n",
    "    df_house = load_dict[key].copy()\n",
    "    df_house['index'] = pd.to_datetime(df_house['index'], unit='s')\n",
    "    df_house.set_index('index', inplace=True)\n",
    "    df_house = df_house[df_house.index > pd.to_datetime(INDEX_START, unit='s')]\n",
    "    df_house = df_house[COLUMNS]\n",
    "    df_summe = df_summe.fillna(0) + df_house.fillna(0)\n",
    "# normieren des Leistungsfaktor    \n",
    "df_summe['PF_TOT'] = df_summe['PF_TOT']/len(load_dict)\n",
    "plot_with_moving_average(df_summe, 'Summe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "correlation_matrix = df_summe.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix')\n",
    "#plt.xlabel('Variablen')\n",
    "#plt.ylabel('Variablen')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Influence of day type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summe.index = pd.to_datetime(df_summe.index, unit='s')\n",
    "\n",
    "df_wochentage = df_summe[df_summe.index.dayofweek < 5]\n",
    "df_wochenende = df_summe[df_summe.index.dayofweek >= 5]\n",
    "\n",
    "weekdays_avg = df_wochentage[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "weekend_avg = df_wochenende[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "\n",
    "# Erstellen eines neuen DataFrames mit den Durchschnittswerten\n",
    "df_avg = pd.DataFrame({'Wochentage': weekdays_avg, 'Wochenende': weekend_avg})\n",
    "\n",
    "# Erstellen eines horizontalen Balkendiagramms\n",
    "fig = go.Figure()\n",
    "\n",
    "# Hinzufügen der Balken für jeden Datensatz\n",
    "for i, col in enumerate(df_avg.columns):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_avg.index,  # Spaltennamen werden auf der y-Achse angezeigt\n",
    "        x=df_avg[col],  # Durchschnittswerte werden auf der x-Achse angezeigt\n",
    "        name=col,  # Legendenname\n",
    "        orientation='h',  # Horizontale Balken\n",
    "        text=df_avg[col].round(2),\n",
    "        textposition='inside'\n",
    "       # marker_color=colors[i]  # Farbe der Balken\n",
    "    ))\n",
    "\n",
    "# Aktualisieren des Layouts für ein gruppiertes Balkendiagramm\n",
    "fig.update_layout(\n",
    "    barmode='group',  # Gruppierung der Balken\n",
    "    title='Durchschnittswerte von Wochentagen und Wochenenden',\n",
    "    title_x = 0.5,\n",
    "    xaxis_title='Durchschnittswerte',\n",
    "    yaxis_title='',\n",
    "    legend_title='Tagtyp',\n",
    "    bargap=0.2,  # Abstand zwischen den Balkengruppen\n",
    "    template='simple_white'\n",
    ")\n",
    "\n",
    "# Anzeigen des Diagramms\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Operation modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > P < 100W: Standby\n",
    "- > 100W < P < 4kW: compressor mode\n",
    "- > P > 4kW: heating rod mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consumtion_type_histo(df_consumptions, years):\n",
    "    df_consumptions.reset_index(inplace=True)\n",
    "    # Erstellen Sie das Balkendiagramm\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Hinzufügen der Balken für jede Kategorie\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Standby'],\n",
    "        name='Stand-by-Modus',\n",
    "        marker_color='green'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Kompressions-Modus'],\n",
    "        name='Kompressions-Modus',\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Heizstab-Modus'],\n",
    "        name='Heizstab-Modus',\n",
    "        marker_color='red'\n",
    "    ))\n",
    "\n",
    "    # Update das Layout\n",
    "    fig.update_layout(\n",
    "        title='Verbrauchte Wirkleistung in kWh/a - {}'.format(years),\n",
    "        title_x = 0.5,\n",
    "        xaxis_tickangle=-45,\n",
    "        xaxis_title='Haushalt',\n",
    "        yaxis_title='Wirkleistung in kWh/a',\n",
    "        barmode='group',\n",
    "        legend_title='Legend',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Zeigen Sie die Figur an\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "#only for columns as index \n",
    "def check_operation_mode(x):\n",
    "    if x < 100:\n",
    "        return 1\n",
    "    elif (x > 100) & (x < 4000):\n",
    "        return 2\n",
    "    elif x >=4000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_operation_mode)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)\n",
    "##########\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in sorted_columns:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions.columns:\n",
    "    df_consumptions[column] = df_consumptions[column]/1000\n",
    "df_consumptions.head()\n",
    "plot_consumtion_type_histo(df_consumptions, '2018-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2018 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2018.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2018]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2018.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2018.columns:\n",
    "    df_consumptions_2018[column] = df_consumptions_2018[column]/1000\n",
    "df_consumptions_2018.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2018, 2018)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2019 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2019.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2019]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2019.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2019.columns:\n",
    "    df_consumptions_2019[column] = df_consumptions_2019[column]/1000\n",
    "df_consumptions_2019.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2019,2019)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2020 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2020.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2020]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2020.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2020.columns:\n",
    "    df_consumptions_2020[column] = df_consumptions_2020[column]/1000\n",
    "df_consumptions_2020.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2020, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Nominal power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump_cleaned_v1.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[]\n",
    "labels=[]\n",
    "for house in load_dict:\n",
    "    values.append(load_dict[house]['P_TOT'].max())\n",
    "    labels.append(house)\n",
    "\n",
    "px.scatter(x=labels, y=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Angenommen, Ihr DataFrame heißt df\n",
    "# df = pd.DataFrame(...)\n",
    "# und 'load_dict' ist ein verfügbarer Dictionary mit Ihren Daten\n",
    "def plot_distribution(df, id):\n",
    "    # Erstellen Sie eine Figur und ein Array von Subplots mit 1 Zeile und 2 Spalten\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "    # Histogramm für die gesamte Spalte 'P_TOT'\n",
    "    axes[0].hist(df['P_TOT'], bins=50, edgecolor='k', alpha=0.7)\n",
    "    axes[0].set_title('Verteilung der Werte in P_TOT')\n",
    "    axes[0].set_xlabel('P_TOT')\n",
    "    axes[0].set_ylabel('Häufigkeit')\n",
    "\n",
    "    # Histogramm für die gefilterten Werte in 'P_TOT' größer als 6000\n",
    "    gefilterte_werte = df[df['P_TOT'] > 7900]['P_TOT']\n",
    "    axes[1].hist(gefilterte_werte, bins=50, edgecolor='k', alpha=0.7)\n",
    "    axes[1].set_title('Verteilung der Werte in P_TOT > 7900')\n",
    "    axes[1].set_xlabel('P_TOT')\n",
    "    axes[1].set_ylabel('Häufigkeit')\n",
    "\n",
    "    # Fügen Sie der ganzen Figur einen Titel hinzu\n",
    "    fig.suptitle('Histogramme der P_TOT Werte - {}'.format(id), fontsize=12)\n",
    "\n",
    "    # Anpassung des Layouts\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Zeigen Sie die Graphiken an\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_small = []\n",
    "list_big = []\n",
    "to_clean = []\n",
    "to_clean_50 = []\n",
    "to_clean_200 = []\n",
    "for key in sorted(load_dict.keys(), key=lambda x: int(x.replace(\"SFH\", \"\"))):\n",
    "    if load_dict[key]['P_TOT'].max() < 7900:\n",
    "        list_small.append(key)\n",
    "        continue\n",
    "    elif len(load_dict[key][load_dict[key]['P_TOT'] > 7900]) < 10:\n",
    "        to_clean.append(key)\n",
    "        list_small.append(key)\n",
    "        continue\n",
    "    elif (len(load_dict[key][load_dict[key]['P_TOT'] > 7900]) < 50):\n",
    "        to_clean_50.append(key)\n",
    "        #list_small.append(key)\n",
    "        continue\n",
    "    elif (len(load_dict[key][load_dict[key]['P_TOT'] > 7900]) < 200):\n",
    "        to_clean_200.append(key)\n",
    "        #list_small.append(key)\n",
    "        continue\n",
    "    list_big.append(key)\n",
    "    print(len(load_dict[key][load_dict[key]['P_TOT'] > 7900]))\n",
    "    plot_distribution(load_dict[key], key)\n",
    "print(\"to clean < 10: {}\".format(to_clean))\n",
    "print(\"to clean < 50: {}\".format(to_clean_50))\n",
    "print(\"to clean < 200: {}\".format(to_clean_200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel('Data/Gebaeudeinformationen.xlsx', header=0)\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[info[\"Number of inhabitants\"]==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[info[\"Number of inhabitants\"]==3].dropna()[\"Building area\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.at[27, \"Building area\"] = info[info[\"Number of inhabitants\"]==3].dropna()[\"Building area\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.iloc[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.to_excel(\"Data/Gebaeudeinformationen_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = False\n",
    "for index in df_dict_2018:\n",
    "    if (len(df_dict_2018[index]['index'])) != 35040:\n",
    "        print(\"issue with index \" + str(index))\n",
    "        checker = True\n",
    "if not checker:\n",
    "    print('all indices have the same size (1,35040)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house in df_dict:\n",
    "    df_dict[house]['time_difference'] = df_dict[house]['index'] - df_dict[house]['index'].shift(1)\n",
    "    if df_dict[house]['time_difference'].value_counts()[900.0] != 105215:\n",
    "        print(house)\n",
    "else:\n",
    "    print('time stamps continues 15 min intervalls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDF5 file, convert to pandas format, concat data for 2018-2020, prepare for use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part contains the code to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read in the weather data in hdf5 format, each year stored in a seperate file, and convert the data format to a python dictionary containing the weather data over the available time span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Some data exploration for the weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Merge all weather data features to one dataframe with continuos 15 min timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Some more data exploration containing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - the visualization of each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a correlation analysis of the parameters, concluding that only 8 of 10 parameters are relevant for further use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4. Additional code used to check code functionality and data quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_to_pandas(hdf_dataset):\n",
    "    column_type_dict = {x:str(y[0]) for x,y in hdf_dataset.dtype.fields.items()}\n",
    "    column_list = []\n",
    "    for index in column_type_dict:\n",
    "        column_list.append(index)\n",
    "    list_of_rows = []\n",
    "    for line in range(0, hdf_dataset.size):\n",
    "        list_of_rows.append(np.asarray(hdf_dataset[line]).tolist())\n",
    "    return pd.DataFrame(data=list_of_rows, columns=column_list)\n",
    "\n",
    "def first_n_digits(num, n):\n",
    "    return num // 10 ** (int(math.log(num, 10)) - n + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in hdf5 data and convert to pandas format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weather data for 2018 to one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Data/HDF5data/weather/2018_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2018 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2018[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2018[key][\"index\"] = weather_dict_2018[key][\"index\"].apply(lambda x: first_n_digits(x, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weather data for 2019 to one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Data/HDF5data/weather/2019_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2019 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2019[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2019[key][\"index\"] = weather_dict_2019[key][\"index\"].apply(lambda x: first_n_digits(x, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weather data for 2020 to one dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Data/HDF5data/weather/2020_weather.hdf5', 'r')\n",
    "dset_weather = file[\"WEATHER_SERVICE\"]\n",
    "dset_weather = dset_weather[\"IN\"]\n",
    "\n",
    "weather_dict_2020 = {}\n",
    "for key in dset_weather:\n",
    "    df_variable = dset_weather[key]\n",
    "    df_variable = df_variable['table']\n",
    "    weather_dict_2020[key] = hdf_to_pandas(df_variable)\n",
    "    \n",
    "    #shorten 64 to 32 bit integer\n",
    "    weather_dict_2020[key][\"index\"] = weather_dict_2020[key][\"index\"].apply(lambda x: first_n_digits(x, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concat weather data, 2018-2020 for each parameter in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict = {}\n",
    "\n",
    "for parameter in weather_dict_2018:\n",
    "    weather_dict[parameter] = pd.concat([weather_dict_2018[parameter],weather_dict_2019[parameter],weather_dict_2020[parameter]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/weather/data_weather.pkl', 'wb') as f:\n",
    "    pickle.dump(weather_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/weather/data_weather.pkl', 'rb') as f:\n",
    "    weather_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Raw data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of available information of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in weather_dict:\n",
    "    print(str(parameter) + \" \" + str(len(weather_dict[parameter])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time resolution for temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'WEATHER_TEMPERATURE_TOTAL'\n",
    "weather_dict_2019[parameter].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict_2019[parameter]['time_difference'] = weather_dict_2019[parameter]['index'] - weather_dict_2019[parameter]['index'].shift(1)\n",
    "weather_dict_2019[parameter]['time_difference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = 'WEATHER_TEMPERATURE_TOTAL'\n",
    "weather_dict[parameter]['time_difference'] = weather_dict[parameter]['index'] - weather_dict[parameter]['index'].shift(1)\n",
    "weather_dict[parameter]['time_difference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = weather_dict[parameter]['time_difference'].value_counts()\n",
    "\n",
    "fig = px.pie(values=value_counts.values, names=value_counts.index, title=\"Häufigkeitsverteilung\")\n",
    "\n",
    "# Anwenden der \"Simply White\" Formatvorlage\n",
    "fig.update_layout(template=\"plotly_white\")\n",
    "\n",
    "# Anzeigen des Diagramms\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Angenommene Werte basierend auf dem hochgeladenen Screenshot für die Demonstration.\n",
    "# Sie würden diese mit den tatsächlichen Werten aus Ihrem value_counts ersetzen.\n",
    "data = {\n",
    "    'time_difference': [300.0, 3600.0, 60.0, 600.0, 0.0, 10.0, 900.0, 1200.0, 2100.0, 1500.0],\n",
    "    'count': [246673, 5323, 3536, 272, 113, 73, 58, 19, 4, 4]\n",
    "}\n",
    "\n",
    "# Erstellung eines DataFrames\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Erstellung des ersten Diagramms\n",
    "top_three = df.nlargest(3, 'count')\n",
    "sum_of_others = pd.DataFrame(data = {\n",
    "    'time_difference': ['Sonstige'],\n",
    "    'count': [df['count'][3:].sum()]\n",
    "})\n",
    "df_top_others = pd.concat([top_three, sum_of_others], ignore_index=True)\n",
    "\n",
    "# Erstellung des Kreisdiagramms für die Top 3 Werte und 'Others'\n",
    "fig1 = px.pie(df_top_others, values='count', names='time_difference', title='Verteilung der zeitlichen Abstände (in Sekunden) zwischen den Messwerten')\n",
    "fig1.update_layout(template=\"plotly_white\", title_x=0.95, legend_x=0.1, )\n",
    "\n",
    "# Erstellung des zweiten Diagramms\n",
    "# Die restlichen Werte ohne die Top 3\n",
    "df_rest = df.iloc[3:]\n",
    "\n",
    "# Erstellung des Kreisdiagramms für die restlichen Werte\n",
    "fig2 = px.pie(df_rest, values='count', names='time_difference', title='Sonstige', )\n",
    "fig2.update_layout(template=\"plotly_white\",title_x=0.5, title_y=0.825,legend_x=0.75)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> No standardized time stamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get load data index as reference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "ref_index = load_dict['SFH10']['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for df_type in weather_dict:\n",
    "    df_ref = ref_index.to_frame().set_index('index')\n",
    "    df_ref[df_type] = np.nan\n",
    "    df_temp = weather_dict[df_type]\n",
    "    for index in ref_index:\n",
    "        sub_df = df_temp[(df_temp['index'] >= index) & (df_temp['index'] <= index+900)]\n",
    "        if sub_df.empty:\n",
    "            #take previous value\n",
    "            df_ref.loc[index][df_type] = df_ref.loc[index-900][df_type]\n",
    "        else:\n",
    "            #take mean value\n",
    "            df_ref.loc[index][df_type] = sub_df.iloc[:,1].mean()\n",
    "    df_list.append(df_ref)\n",
    "weather_data = pd.concat(df_list, axis=1)\n",
    "with open('Data/weather/data_weather_merged.pkl', 'wb') as f:\n",
    "    pickle.dump(weather_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/weather/data_weather_merged.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Weather data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.DataFrame(columns=weather_data.columns, index=['min', 'max', 'mean', 'median', 'missing values'])\n",
    "for column in weather_data.columns:\n",
    "    df_analysis.loc['min'][column] = weather_data[column].min()\n",
    "    df_analysis.loc['max'][column] = weather_data[column].max()\n",
    "    df_analysis.loc['mean'][column] = weather_data[column].mean()\n",
    "    df_analysis.loc['median'][column] = weather_data[column].median()\n",
    "    df_analysis.loc['missing values'][column] = len(weather_data) - weather_data[column].value_counts().sum()\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plots = weather_data.copy()\n",
    "data_plots.reset_index(inplace=True)\n",
    "data_plots['index'] = pd.to_datetime(data_plots['index'], unit='s')\n",
    "data_plots.set_index('index', inplace=True)\n",
    "\n",
    "fig, a = plt.subplots(5, 2, figsize=(20, 20), tight_layout=True)\n",
    "data_plots.plot(ax=a, subplots=True, rot=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korrelation zwischen den einzelnen Wetterparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dict = {\n",
    "    'WEATHER_APPARENT_TEMPERATURE_TOTAL':           'Scheintemperatur',\n",
    "    'WEATHER_ATMOSPHERIC_PRESSURE_TOTAL':           'Luftdruck',\n",
    "    'WEATHER_PRECIPITATION_RATE_TOTAL':             'Niederschlag',\n",
    "    'WEATHER_PROBABILITY_OF_PRECIPITATION_TOTAL':   'Niederschlagswahrscheinlichkeit',\n",
    "    'WEATHER_RELATIVE_HUMIDITY_TOTAL':              'Relative Luftfeuchtigkeit',\n",
    "    'WEATHER_SOLAR_IRRADIANCE_GLOBAL':              'Sonneneinstrahlung',\n",
    "    'WEATHER_TEMPERATURE_TOTAL':                    'Temperatur',\n",
    "    'WEATHER_WIND_DIRECTION_TOTAL':                 'Windrichtung',\n",
    "    'WEATHER_WIND_GUST_SPEED_TOTAL':                'Windböenstärke',\n",
    "    'WEATHER_WIND_SPEED_TOTAL':                     'Windgeschwindigkeit'\n",
    "}\n",
    "\n",
    "\n",
    "correlation_matrix = weather_data.rename(columns=columns_dict).corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix')\n",
    "#plt.xlabel('Variablen')\n",
    "#plt.ylabel('Variablen')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Entfernen der Scheintemperatur sowie der Windböenstärke, da diese von der Absoluttemperatur sowie der Windgeschwindigkeit bereits gut erfasst werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_weather_data = weather_data.drop(columns=['WEATHER_APPARENT_TEMPERATURE_TOTAL', 'WEATHER_WIND_GUST_SPEED_TOTAL'])\n",
    "with open('Data/weather/data_weather_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(reduced_weather_data, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
