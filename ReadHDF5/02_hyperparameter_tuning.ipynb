{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameteroptimierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieses Skript dient zum Finden der passenden Hyperparameter zum Trainieren des GRU-Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle \n",
    "import re\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from kerastuner import HyperModel, RandomSearch, HyperParameters\n",
    "\n",
    "from utils import create_daily_sequences, test_sequences, get_optimization_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump_cleaned_v1.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "with open('Data/weather/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)\n",
    "\n",
    "building_info = pd.read_excel(\"Data/Gebaeudeinformationen_cleaned.xlsx\", index_col=0)\n",
    "building_info.set_index(\"Building number\", inplace=True)\n",
    "\n",
    "# add building information\n",
    "for house in load_dict:\n",
    "    id = int(re.findall(r'\\d+', house)[0])\n",
    "\n",
    "    load_dict[house][\"area\"] = building_info.loc[id][\"Building area\"]\n",
    "    load_dict[house][\"inhabitants\"] = building_info.loc[id][\"Number of inhabitants\"]\n",
    "    load_dict[house][\"building\"] = id\n",
    "    \n",
    "    weather_data = weather_data[weather_data.index>=1528965900]\n",
    "    load_dict[house] = pd.concat([load_dict[house], weather_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwendung eines Datensatu, um Berechnungszeit einzusparen\n",
    "df = load_dict[\"SFH23\"] \n",
    "\n",
    "# Skalierung der Daten\n",
    "len_input_seq = 3       # in Tagen\n",
    "len_output_seq = 1      # in Tagen\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "X, y = create_daily_sequences(df_scaled, len_input_seq, len_output_seq)\n",
    "test_sequences(X, y, len_input_seq*96, len_output_seq*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings und Validierungsdatensatz\n",
    "\n",
    "train_size = int(len(X) * 0.8)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size], y[train_size:]\n",
    "\n",
    "# Anpassen des Zielvariablen-Formats an Modellarchitektur (192 Ausgabeneuronen, keine zweidimensionaler Aufbau)\n",
    "y_train_reshaped = y_train.reshape(-1, 96 * 2)  # Umformen in [Anzahl der Beispiele, 192]\n",
    "y_val_reshaped = y_val.reshape(-1, 96 * 2)      # Umformen in [Anzahl der Beispiele, 192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierungsansatz 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, output_units):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        for i in range(hp.Int('num_gru_layers', 1, 5)):\n",
    "            if i == 0:\n",
    "                # Nur für die erste Schicht wird input_shape gesetzt\n",
    "                model.add(GRU(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                              return_sequences=True if hp.get('num_gru_layers') > 1 else False,\n",
    "                              input_shape=self.input_shape))\n",
    "            else:\n",
    "                # Für nachfolgende Schichten wird input_shape nicht gesetzt\n",
    "                model.add(GRU(units=hp.Int(f'units_{i}', min_value=32, max_value=256, step=32),\n",
    "                              return_sequences=True if i < hp.get('num_gru_layers') - 1 else False))\n",
    "\n",
    "            model.add(Dropout(hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "        model.add(Dense(self.output_units, activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "        # Anpassung der Batch-Größe als Hyperparameter\n",
    "        kwargs['batch_size'] = hp.Choice('batch_size', values=[32, 64, 128])\n",
    "        return model.fit(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe- und Ausgabe-Shape für das Modell\n",
    "input_shape = (None, 13)    # Zum Beispiel (Sequenzlänge, Anzahl der Merkmale)\n",
    "output_units = 96 * 2       # Zum Beispiel (Sequenzlänge * Anzahl der Zielvariablen)\n",
    "tuner = RandomSearch(\n",
    "    GRUHyperModel(input_shape, output_units),\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='gru_hyperparam_tuning_3'\n",
    ")\n",
    "\n",
    "tuner.search(x=X_train, \n",
    "             y=y_train_reshaped, \n",
    "             epochs=10, \n",
    "             validation_data=(X_val, y_val_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_optimization_results(tuner, 20)\n",
    "results[results['val_loss'] == results['val_loss'].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimierungsansatz 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape, output_units):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_units = output_units\n",
    "\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Eingabeschicht\n",
    "        model.add(GRU(\n",
    "            units=hp.get('units_0'),\n",
    "            return_sequences=hp.get('num_gru_layers') > 1,\n",
    "            input_shape=self.input_shape\n",
    "        ))\n",
    "        model.add(Dropout(hp.get('dropout_rate_0')))\n",
    "        \n",
    "        # Weitere GRU Schichten\n",
    "        for i in range(1, hp.get('num_gru_layers')):\n",
    "            model.add(GRU(\n",
    "                units=hp.get(f'units_{i}'),\n",
    "                return_sequences=i < hp.get('num_gru_layers') - 1\n",
    "            ))\n",
    "            model.add(Dropout(hp.get(f'dropout_rate_{i}')))\n",
    "\n",
    "        # Ausgabeschicht\n",
    "        model.add(Dense(self.output_units, activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Festlegen der Hyperparameter außerhalb des Hypermodells\n",
    "hp = HyperParameters()\n",
    "hp.Int('num_gru_layers', 1, 5, default=2)\n",
    "hp.Int('units_0', 32, 256, step=32)\n",
    "hp.Float('dropout_rate_0', 0, 0.5, step=0.1)\n",
    "for i in range(1, 5):  # Maximal 5 Schichten\n",
    "    hp.Int(f'units_{i}', 32, 256, step=32, default=32)\n",
    "    hp.Float(f'dropout_rate_{i}', 0, 0.5, step=0.1, default=0.1)\n",
    "hp.Choice('batch_size', values=[32, 64, 128])\n",
    "\n",
    "# Eingabe- und Ausgabe-Shape für das Modell\n",
    "input_shape = (None, 13)  # Zum Beispiel (Sequenzlänge, Anzahl der Merkmale)\n",
    "output_units = 96 * 2  # Zum Beispiel (Sequenzlänge * Anzahl der Zielvariablen)\n",
    "\n",
    "# Initialisieren des Tuners\n",
    "tuner = RandomSearch(\n",
    "    GRUHyperModel(input_shape, output_units),\n",
    "    hyperparameters=hp,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_dir',\n",
    "    project_name='gru_hyperparam_tuning_6'\n",
    ")\n",
    "\n",
    "# Starten des Suchvorgangs\n",
    "tuner.search(\n",
    "    x=X_train,\n",
    "    y=y_train_reshaped,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    batch_size=hp.get('batch_size')  # Verwendung der Batch-Größe aus den Hyperparametern\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_optimization_results(tuner, 20)\n",
    "results[results['val_loss'] == results['val_loss'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Erstellen eines Lineplots mit Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results.index,\n",
    "    y=results['val_loss'],\n",
    "    mode='lines+markers',\n",
    "    name='Validierungsverlust'\n",
    "))\n",
    "\n",
    "# Update layout für Titel und Gitternetz\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'Entwicklung des Validierungsverlustes über die Trials',\n",
    "        'y':0.9,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'\n",
    "    },\n",
    "    xaxis_title='Trial Index',\n",
    "    yaxis_title='Validierungsverlust',\n",
    "    template='simple_white',\n",
    "    xaxis=dict(showgrid=True),  # Gitternetz für die X-Achse\n",
    "    yaxis=dict(showgrid=True)   # Gitternetz für die Y-Achse\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
