{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from utils.utils import create_daily_sequences, test_sequences, get_optimization_results, plot_validation_loss, print_metrics, quick_result_plot, calculate_nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump_cleaned_v1.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "with open('Data/weather/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)\n",
    "\n",
    "building_info = pd.read_excel(\"Data/Gebaeudeinformationen_cleaned.xlsx\", index_col=0)\n",
    "building_info.set_index(\"Building number\", inplace=True)\n",
    "\n",
    "# add building information\n",
    "for house in load_dict:\n",
    "    id = int(re.findall(r'\\d+', house)[0])\n",
    "\n",
    "    load_dict[house][\"area\"] = building_info.loc[id][\"Building area\"]\n",
    "    load_dict[house][\"inhabitants\"] = building_info.loc[id][\"Number of inhabitants\"]\n",
    "    load_dict[house][\"building\"] = id\n",
    "    \n",
    "    weather_data = weather_data[weather_data.index>=1528965900]\n",
    "    load_dict[house] = pd.concat([load_dict[house], weather_data], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globale Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENZE_LENGTH = 3  # in Tagen\n",
    "PREDICTION_LENGTH = 1 # in Tagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisierung des Scalers\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Listen für X und y arrays\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for building_id in tqdm(load_dict, desc=\"Verarbeite Gebäude\"):\n",
    "    # Skalieren der Werte\n",
    "    load_dict[building_id] = pd.DataFrame(scaler.fit_transform(load_dict[building_id]), columns=load_dict[building_id].columns)\n",
    "    # Erstellen der Sequenzen\n",
    "    X_building, y_building = create_daily_sequences(load_dict[building_id], SEQUENZE_LENGTH, PREDICTION_LENGTH)\n",
    "    # Hinzufügen der Sequenzen zur Gesamtliste\n",
    "    all_X.append(X_building)\n",
    "    all_y.append(y_building)\n",
    "\n",
    "# Für Tages Sequenzen\n",
    "X = np.concatenate(all_X, axis=0)\n",
    "y = np.concatenate(all_y, axis=0)\n",
    "\n",
    "print(\"Dimensionen X: \" + str(X.shape))\n",
    "print(\"Dimensionen y: \" + str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences(X, y, SEQUENZE_LENGTH*96, PREDICTION_LENGTH*96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings-, Test- und Validierungsdaten\n",
    "# Ansatz: Chronologische Aufteilung ohne Überlappung\n",
    "# sonst: sklearn.train_test_split() mit zufälliger Aufteilung der Daten -> Problem: Vorhersage der Vergangenheit mit Werten aus der Zukunft?\n",
    "\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.85)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:val_size], y[train_size:val_size]\n",
    "X_test, y_test = X[val_size:], y[val_size:]\n",
    "\n",
    "df_test_start = pd.DataFrame()\n",
    "df_test_start['P_TOT'] = y_test[:, :, 0].flatten()\n",
    "df_test_start['PF_TOT'] = y_test[:, :, 1].flatten()\n",
    "\n",
    "y_train = y_train.reshape(-1, 96 * 2)   # Umformen in [Anzahl der Beispiele, 192]\n",
    "y_val= y_val.reshape(-1, 96 * 2)        # Umformen in [Anzahl der Beispiele, 192]\n",
    "y_test = y_test.reshape(-1, 96 * 2)     # Umformen in [Anzahl der Beispiele, 192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modellarchitektur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_0 = 192\n",
    "num_gru_layers = 4\n",
    "hidden_units = [128, 96, 256, 160]\n",
    "units_outputs = 96 * 2\n",
    "dropout_rate_0 = 0.3\n",
    "dropout_hidden = [0.3, 0.3, 0.2, 0.2]\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Eingabeschicht\n",
    "model.add(GRU(units=units_0,            # Hyperparameter -> kann variiert und angepasst werden\n",
    "              return_sequences=True,    # Konfigurationsparameter, default: False\n",
    "                                        # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                        #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                        #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                        #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                        # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                        #            ansonsten: return_sequences=False.\n",
    "                                        # \n",
    "              input_shape=(\n",
    "                  X.shape[1],           # Sequenzlänge\n",
    "                  X.shape[2]            # Anzahl der Features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "# Dropout-Schicht\n",
    "model.add(Dropout(dropout_rate_0))  # Dropout zur Vermeidung von Overfitting durch zufälliges Deaktivieren von Neuronen während des Trainingsprozesses -> Vermeidung dominanter Neuronen -> bessere Generalisierung\n",
    "\n",
    "# Weitere GRU-Schichten\n",
    "for i in range(0, num_gru_layers):\n",
    "    model.add(GRU(units=hidden_units[i],\n",
    "                  return_sequences= i < (num_gru_layers - 1)    # True für Schichten 1-3, False für Schicht 4\n",
    "                )\n",
    "            )\n",
    "    model.add(Dropout(dropout_hidden[i]))\n",
    "\n",
    "# Ausgabeschicht\n",
    "model.add(Dense(units_outputs, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Modellstruktur\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "model.compile(\n",
    "    optimizer='adam',                   # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                        # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                        # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                        #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                        #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                        #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                        #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                        #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                        # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "    loss='mean_squared_error',          # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "    metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Modells\n",
    "history = model.fit(\n",
    "    X_train, y_train,                   # Übergabe der Trainingsdaten\n",
    "    epochs=100,                         # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                        #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "    batch_size=batch_size,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                        #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "    validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "    use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "    workers=4,                          # Nutzen mehrerer CPU-Kerne\n",
    "    verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/GRU_4layer_1.h5')  # Speichert das Modell im HDF5-Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_loss(history, \"- Model 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluierung des Modells mit den Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/GRU_4layer_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print('Testverlust:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorhersagen auf den Testdaten machen\n",
    "predicted_test = model.predict(X_test)\n",
    "print(\"Shape predicted_test {}\".format(predicted_test.shape))\n",
    "print(\"Shape y_test {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(y_test, predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "print_metrics(y_test[n], predicted_test[n])\n",
    "print(\"nRMSE: \" + str(calculate_nrmse(y_test[n], predicted_test[n])))\n",
    "quick_result_plot(predicted_test[n], y_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "mape_dict = {}\n",
    "nrsme_dict = {}\n",
    "for i in range(1, len(X_test)): \n",
    "    mape = mean_absolute_percentage_error(y_test[i-1:i], predicted_test[i-1:i])\n",
    "    nrsme = calculate_nrmse(y_test[i-1:i], predicted_test[i-1:i])\n",
    "    mape_dict[i] = mape\n",
    "    nrsme_dict[i] = nrsme\n",
    "\n",
    "results = pd.Series(nrsme_dict).to_frame().rename(columns={0:\"nRSME\"})\n",
    "results[\"MAPE\"] = pd.Series(mape_dict)\n",
    "results[\"nRSME\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results[\"MAPE\"]>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rückskalierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape y_test zurück in die ursprüngliche Form [Anzahl der Beispiele, 96 Zeitpunkte, 2 Variablen]\n",
    "predicted_test_reshaped = predicted_test.reshape(-1, 96, 2)\n",
    "y_test_reshaped = y_test.reshape(-1, 96, 2)\n",
    "\n",
    "# Erstellen eines leeren DataFrames für die Ergebnisse\n",
    "df_pred = pd.DataFrame()\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "# Extrahieren und zuordnen der Daten zu den entsprechenden Spalten\n",
    "df_pred['P_TOT_pred'] = predicted_test_reshaped[:, :, 0].flatten()\n",
    "df_pred['PF_TOT_pred'] = predicted_test_reshaped[:, :, 1].flatten()\n",
    "df_test['P_TOT'] = y_test_reshaped[:, :, 0].flatten()\n",
    "df_test['PF_TOT'] = y_test_reshaped[:, :, 1].flatten()\n",
    "\n",
    "# Erstellen eines temporären DataFrame mit der gleichen Struktur wie df_original_structure\n",
    "temp_df = pd.DataFrame(0, index=np.arange(len(df_pred)), columns=load_dict[\"SFH10\"].columns)\n",
    "# Setzen der Werte für die Zielvariablen\n",
    "temp_df['P_TOT'] = df_pred['P_TOT_pred']\n",
    "temp_df['PF_TOT'] = df_pred['PF_TOT_pred']\n",
    "# Anwenden von inverse_transform\n",
    "temp_array = scaler.inverse_transform(temp_df)\n",
    "# Erstellen eines neuen DataFrame mit den invers transformierten Werten\n",
    "df_inverse_transformed = pd.DataFrame(temp_array, columns=load_dict[\"SFH10\"].columns)\n",
    "# Extrahieren der invers transformierten Zielvariablen\n",
    "predictions = df_inverse_transformed[['P_TOT', 'PF_TOT']]\n",
    "#predictions.index = df[int(len(df)*0.85):].index[:360864]\n",
    "\n",
    "# Erstellen eines temporären DataFrame mit der gleichen Struktur wie df_original_structure\n",
    "temp_df = pd.DataFrame(0, index=np.arange(len(df_test)), columns=load_dict[\"SFH10\"].columns)\n",
    "# Setzen der Werte für die Zielvariablen\n",
    "temp_df['P_TOT'] = df_test['P_TOT']\n",
    "temp_df['PF_TOT'] = df_test['PF_TOT']\n",
    "# Anwenden von inverse_transform\n",
    "temp_array = scaler.inverse_transform(temp_df)\n",
    "# Erstellen eines neuen DataFrame mit den invers transformierten Werten\n",
    "df_inverse_transformed = pd.DataFrame(temp_array, columns=load_dict[\"SFH10\"].columns)\n",
    "# Extrahieren der invers transformierten Zielvariablen\n",
    "test_data = df_inverse_transformed[['P_TOT', 'PF_TOT']]\n",
    "#test_data.index = df[int(len(df)*0.85):].index[:360864]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_nrmse(test_data[\"P_TOT\"], predictions[\"P_TOT\"]))\n",
    "print(calculate_nrmse(test_data[\"PF_TOT\"], predictions[\"PF_TOT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calculate_nrmse(y_test[:, 0:96], predicted_test[:, 0:96]))\n",
    "print(calculate_nrmse(y_test[:, 96:], predicted_test[:, 96:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_percentage_error(y_test[:, 0:96], predicted_test[:, 0:96]))\n",
    "print(mean_absolute_percentage_error(test_data[\"P_TOT\"], predictions[\"P_TOT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_absolute_percentage_error(y_test[:, 96:], predicted_test[:, 96:]))\n",
    "print(mean_absolute_percentage_error(test_data[\"PF_TOT\"], predictions[\"PF_TOT\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_start.equals(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
