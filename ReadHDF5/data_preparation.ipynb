{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script contains code to complete the heatpump load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore two approaches are implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        1) Using Linear Regression to interpolate missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        2) Further reduction of data sample to reach full data availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_functions import plot_consumption_resampled, plot_metrics_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START = 1528965000\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "with open('Data/weather/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='Data/data_availability>85.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "# set index to start index\n",
    "for key in load_dict:\n",
    "    df_house = load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index > INDEX_START]\n",
    "    df_house = df_house[COLUMNS]\n",
    "\n",
    "    for column in df_house.columns:\n",
    "        if not df_house[df_house[column]<0].empty:\n",
    "            df_house.loc[df_house[column] < 0, column] = 0.01\n",
    "\n",
    "    load_dict[key] = df_house    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 1: Regression model for filling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution 2: Further reduction, remove SFH 10, 11, and 23. New time horizon Nov 18 - Dez 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Solution 3: Using removed data sets to fill missing values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Regression model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incomplete time series\n",
    "list_complete = ['SFH12', 'SFH14', 'SFH16', 'SFH18', 'SFH19', 'SFH22', 'SFH27', 'SFH28', 'SFH29', \n",
    "                 'SFH3', 'SFH30', 'SFH32', 'SFH34', 'SFH36', 'SFH4', 'SFH9', 'SFH26', 'SFH33']\n",
    "list_incomplete = ['SFH5', 'SFH7', 'SFH10', 'SFH11', 'SFH20', 'SFH21', 'SFH23', 'SFH38', 'SFH39']\n",
    "list_incomlete_unique = ['SFH5', 'SFH7', 'SFH10', 'SFH11', 'SFH21', 'SFH38', 'SFH39']\n",
    "list_incomplete_double = ['SFH20', 'SFH23']\n",
    "list_v1 = list_complete + list_incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/missing_intervalls_dict.pkl', 'rb') as f:\n",
    "    missing_intervalls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(load_dict['SFH3'].index[0], unit='s')\n",
    "end = pd.to_datetime(load_dict['SFH3'].index[-1], unit='s')\n",
    "\n",
    "end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house = load_dict['SFH39']\n",
    "\n",
    "data = pd.merge(left=df_house, right=weather_data, how='inner', left_on=df_house.index, right_on=weather_data.index)\n",
    "data.rename(columns={'key_0':'index'}, inplace=True)\n",
    "data.set_index('index', inplace=True)\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['P_TOT','PF_TOT', 'WEATHER_TEMPERATURE_TOTAL', 'WEATHER_PRECIPITATION_RATE_TOTAL', 'WEATHER_WIND_SPEED_TOTAL']\n",
    "correlation_matrix = data[columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Korrelationsmatrix')\n",
    "#plt.xlabel('Variablen')\n",
    "#plt.ylabel('Variablen')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "\n",
    "data_columns = ['P_TOT', 'PF_TOT', 'WEATHER_TEMPERATURE_TOTAL', 'WEATHER_PRECIPITATION_RATE_TOTAL', 'WEATHER_WIND_SPEED_TOTAL']\n",
    "weather_columns = ['WEATHER_TEMPERATURE_TOTAL', 'WEATHER_PRECIPITATION_RATE_TOTAL', 'WEATHER_WIND_SPEED_TOTAL']    \n",
    "\n",
    "# set index to start index\n",
    "for key in load_dict:\n",
    "    df_house =load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index > INDEX_START]\n",
    "    df_house = df_house[['P_TOT', 'PF_TOT']]\n",
    "\n",
    "    for column in df_house.columns:\n",
    "        if not df_house[df_house[column]<0].empty:\n",
    "            df_house.loc[df_house[column] < 0, column] = 0\n",
    "\n",
    "    load_dict[key] = df_house    \n",
    "\n",
    "dict_result = {}\n",
    "\n",
    "df_metrics_r = pd.DataFrame(columns=['RMSE', 'MSE', 'R2'], index=list_incomplete)\n",
    "\n",
    "for key in list_incomplete:\n",
    "    # get load data for house\n",
    "    df_house = load_dict[key]\n",
    "\n",
    "    # merge weather and load data to one dataset - train and test data\n",
    "    data = pd.merge(left=df_house, right=weather_data, how='inner', left_on=df_house.index, right_on=weather_data.index)\n",
    "    data.rename(columns={'key_0':'index'}, inplace=True)\n",
    "    data.set_index('index', inplace=True)\n",
    "    data = data[data_columns]\n",
    "\n",
    "    # create time features\n",
    "    #data['minute'] = pd.to_datetime(data.index, unit='s').minute\n",
    "    #data['hour'] = pd.to_datetime(data.index, unit='s').hour\n",
    "    #data['day'] = pd.to_datetime(data.index, unit='s').day\n",
    "    #data['month'] = pd.to_datetime(data.index, unit='s').month\n",
    "    #data['year'] = pd.to_datetime(data.index, unit='s').year\n",
    "\n",
    "    # create dataset for prediction, weather data and time features\n",
    "    intervalls = missing_intervalls[key]\n",
    "\n",
    "    for intervall in intervalls:\n",
    "        features_to_predict = weather_data.loc[intervall[0]: intervall[1]]\n",
    "        features_to_predict = features_to_predict[weather_columns]\n",
    "        #features_to_predict['minute'] = pd.to_datetime(features_to_predict.index, unit='s').minute\n",
    "        #features_to_predict['hour'] = pd.to_datetime(features_to_predict.index, unit='s').hour\n",
    "        #features_to_predict['day'] = pd.to_datetime(features_to_predict.index, unit='s').day\n",
    "        #features_to_predict['month'] = pd.to_datetime(features_to_predict.index, unit='s').month\n",
    "        #features_to_predict['year'] = pd.to_datetime(features_to_predict.index, unit='s').year\n",
    "    \n",
    "    #train model\n",
    "        data.dropna(inplace=True)\n",
    "        X = data[data.columns[2:]]\n",
    "        y = data[data.columns[0:2]]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        # Lineare Regression \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        predictions = model.predict(X_test)\n",
    "    \n",
    "        # evaluation of model performance\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = mean_squared_error(y_test, predictions, squared=True)\n",
    "        mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "        #print(key)\n",
    "        #print('The r2 is: ', r2)\n",
    "        #print('The rmse is: ', np.sqrt(rmse))\n",
    "        #print(\"--------------------\")\n",
    "\n",
    "        predictions= model.predict(features_to_predict)\n",
    "        # add data to dataframe\n",
    "        df_house.loc[intervall[0]:intervall[-1]] = predictions\n",
    "    df_metrics_r.loc[key] = [np.sqrt(rmse), mse, r2]\n",
    "   \n",
    "    # add dataframe to dict\n",
    "    dict_result[key] = df_house\n",
    "\n",
    "df_metrics_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of all variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "# set index to start index\n",
    "for key in load_dict:\n",
    "    df_house =load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index > INDEX_START]\n",
    "    df_house = df_house[['P_TOT', 'PF_TOT']]\n",
    "\n",
    "    for column in df_house.columns:\n",
    "        if not df_house[df_house[column]<0].empty:\n",
    "            df_house.loc[df_house[column] < 0, column] = 0\n",
    "\n",
    "    load_dict[key] = df_house    \n",
    "\n",
    "dict_result = {}\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['RMSE', 'MSE', 'R2'], index=list_incomplete)\n",
    "\n",
    "for key in list_incomplete:\n",
    "    # get load data for house\n",
    "    df_house = load_dict[key]\n",
    "\n",
    "    # merge weather and load data to one dataset - train and test data\n",
    "    data = pd.merge(left=df_house, right=weather_data, how='inner', left_on=df_house.index, right_on=weather_data.index)\n",
    "    data.rename(columns={'key_0':'index'}, inplace=True)\n",
    "    data.set_index('index', inplace=True)\n",
    "\n",
    "    # create time features\n",
    "    data['minute'] = pd.to_datetime(data.index, unit='s').minute\n",
    "    data['hour'] = pd.to_datetime(data.index, unit='s').hour\n",
    "    data['day'] = pd.to_datetime(data.index, unit='s').day\n",
    "    data['month'] = pd.to_datetime(data.index, unit='s').month\n",
    "    data['year'] = pd.to_datetime(data.index, unit='s').year\n",
    "\n",
    "    # create dataset for prediction, weather data and time features\n",
    "    intervalls = missing_intervalls[key]\n",
    "\n",
    "    for intervall in intervalls:\n",
    "        features_to_predict = weather_data.loc[intervall[0]: intervall[1]]\n",
    "        features_to_predict['minute'] = pd.to_datetime(features_to_predict.index, unit='s').minute\n",
    "        features_to_predict['hour'] = pd.to_datetime(features_to_predict.index, unit='s').hour\n",
    "        features_to_predict['day'] = pd.to_datetime(features_to_predict.index, unit='s').day\n",
    "        features_to_predict['month'] = pd.to_datetime(features_to_predict.index, unit='s').month\n",
    "        features_to_predict['year'] = pd.to_datetime(features_to_predict.index, unit='s').year\n",
    "\n",
    "        #train model\n",
    "        data.dropna(inplace=True)\n",
    "        X = data[data.columns[2:]]\n",
    "        y = data[data.columns[0:2]]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        # Lineare Regression \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # predict\n",
    "        predictions = model.predict(X_test)\n",
    "    \n",
    "        # evaluation of model performance\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = mean_squared_error(y_test, predictions, squared=True)\n",
    "        mse = mean_squared_error(y_test, predictions, squared=False)\n",
    "\n",
    "        predictions= model.predict(features_to_predict)\n",
    "        # add data to dataframe\n",
    "        df_house.loc[intervall[0]:intervall[-1]] = predictions\n",
    "    df_metrics.loc[key] = [np.sqrt(rmse), mse, r2]\n",
    "   \n",
    "    # add dataframe to dict\n",
    "    dict_result[key] = df_house\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics_lr(df_metrics)\n",
    "plot_metrics_lr(df_metrics_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list_incomplete:\n",
    "    dict_result[key] = dict_result[key].clip(lower=0)\n",
    "    plot_consumption_resampled(dict_result[key], ['P_TOT', 'PF_TOT'], key, missing_intervalls[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add already complete time series\n",
    "for key in list_complete:\n",
    "    dict_result[key] = load_dict[key]\n",
    "# save to file\n",
    "with open('Data/heatpump/data_heatpump_cleaned_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(dict_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Further reduction of data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime(1542512700, unit='s')\n",
    "end = pd.to_datetime(load_dict['SFH3'].index[-1], unit='s')\n",
    "\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "load_dict = {}\n",
    "\n",
    "# set index to start index\n",
    "for key in list_v1:\n",
    "    if key in ['SFH10', 'SFH11', 'SFH23']:\n",
    "        #drop datasets\n",
    "        continue\n",
    "    else:\n",
    "        df_house =data[key].set_index('index')\n",
    "        # start index after missing values for SFH7\n",
    "        df_house = df_house[df_house.index > missing_intervalls['SFH7'][0][1]]\n",
    "        df_house = df_house[COLUMNS]\n",
    "        df_house = df_house.clip(lower=0)\n",
    "\n",
    "        load_dict[key] = df_house   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in load_dict:\n",
    "    plot_consumption_resampled(load_dict[key], ['P_TOT', 'PF_TOT'], key, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('Data/heatpump/data_heatpump_cleaned_v2.pkl', 'wb') as f:\n",
    "    pickle.dump(load_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not working, als backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {\n",
    "    'SFH39': 'SFH31',\n",
    "    'SFH38': 'SFH35',\n",
    "    'SFH23': 'SFH37',\n",
    "    'SFH21': 'SFH6',\n",
    "    'SFH20': 'SFH37',\n",
    "    'SFH11': 'SFH39',\n",
    "    'SFH10': 'SFH40',\n",
    "    'SFH7': 'SFH31',\n",
    "    'SFH5': 'SFH35'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "with open('Data/missing_intervalls_dict.pkl', 'rb') as f:\n",
    "    missing_intervalls_dict = pickle.load(f)\n",
    "\n",
    "load_dict_complete = {}\n",
    "for key in mapping_dict:\n",
    "    if key in ['SFH20', 'SFH23', 'SFH5']:\n",
    "        continue\n",
    "    df_house = load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index >= INDEX_START]\n",
    "    missing_intervalls = missing_intervalls_dict[key][0]\n",
    "    df_replace = load_dict[mapping_dict[key]].set_index('index')\n",
    "    df_replace = df_replace[(df_replace.index >= missing_intervalls[0])&(df_replace.index <= missing_intervalls[1])]\n",
    "    df_house.loc[missing_intervalls[0]:missing_intervalls[1]] = df_replace\n",
    "\n",
    "    load_dict_complete[key] = df_house[COLUMNS]\n",
    "\n",
    "for key in list_complete:\n",
    "    df_house = load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index >= INDEX_START]\n",
    "    load_dict_complete[key] = df_house[COLUMNS]\n",
    "\n",
    "for key in ['SFH20', 'SFH23', 'SFH5']:\n",
    "    df_house = load_dict[key].set_index('index')\n",
    "    df_house = df_house[df_house.index >= IndentationError]\n",
    "    missing_intervalls = missing_intervalls_dict[key]\n",
    "    for intervall in missing_intervalls:\n",
    "        df_replace = load_dict[mapping_dict[key]].set_index('index')\n",
    "        df_replace = df_replace[(df_replace.index >= intervall[0])&(df_replace.index <= intervall[1])]\n",
    "        df_house.loc[intervall[0]:intervall[1]] = df_replace\n",
    "\n",
    "    load_dict_complete[key] = df_house[COLUMNS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = load_dict_complete['SFH10'].reset_index()['index'].to_frame().set_index('index')\n",
    "for df in load_dict_complete:\n",
    "    load_dict_complete[df][df] = load_dict_complete[df]['P_TOT'].apply(check_nan)\n",
    "    df_result = pd.concat([df_result, load_dict_complete[df][df]], axis=1)\n",
    "#df_result.set_index('index', inplace=True)\n",
    "plot_data_availability(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soltion 1: Dev with example house 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = load_dict[list_incomplete[0]]\n",
    "df_5_train = df_5.dropna()\n",
    "df_5_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(left=df_5_train, right=weather_data, how='inner', left_on=df_5_train.index, right_on=weather_data.index)\n",
    "data.rename(columns={'key_0':'index'}, inplace=True)\n",
    "data.set_index('index', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding time dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['minute'] = pd.to_datetime(data.index, unit='s').minute\n",
    "data['hour'] = pd.to_datetime(data.index, unit='s').hour\n",
    "data['day'] = pd.to_datetime(data.index, unit='s').day\n",
    "data['month'] = pd.to_datetime(data.index, unit='s').month\n",
    "data['year'] = pd.to_datetime(data.index, unit='s').year\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weather data for to predicted time horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervalls = missing_intervalls[list_incomplete[0]][0]\n",
    "print(intervalls)\n",
    "index_to_predict = df_5[df_5.isna().any(axis=1)].index\n",
    "print(index_to_predict[0])\n",
    "print(index_to_predict[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_predict = weather_data.loc[index_to_predict[0]: index_to_predict[-1]]\n",
    "features_to_predict['minute'] = pd.to_datetime(features_to_predict.index, unit='s').minute\n",
    "features_to_predict['hour'] = pd.to_datetime(features_to_predict.index, unit='s').hour\n",
    "features_to_predict['day'] = pd.to_datetime(features_to_predict.index, unit='s').day\n",
    "features_to_predict['month'] = pd.to_datetime(features_to_predict.index, unit='s').month\n",
    "features_to_predict['year'] = pd.to_datetime(features_to_predict.index, unit='s').year\n",
    "features_to_predict#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression with scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[data.columns[4:]]\n",
    "y = data[data.columns[0:4]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Lineare Regression Modell erstellen und trainieren\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen treffen\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, predictions)\n",
    "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
    "print('The r2 is: ', r2)\n",
    "print('The rmse is: ', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features_to_predict)\n",
    "df_5.loc[intervalls[0]:intervalls[-1]] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_consumption_resampled(df_5, 'SFH5', intervalls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
