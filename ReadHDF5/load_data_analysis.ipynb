{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDF5 file, convert to pandas format, concat data for 2018-2020, prepare for use, analysis of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains the code to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read in the load pump data for 36 houses in hdf5 format, each year stored in a seperate file and convert the data format to a python dictionary containing the load data of each house over the available time span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Analysis of data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data availabilty\n",
    "- visualizations of seperate load profiles\n",
    "- visualizations of aggregated load profiles\n",
    "- influence of week day type\n",
    "- operating mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Further project information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Globale Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_START = 1525270500\n",
    "INDEX_START_2 = 1528965000\n",
    "COLUMNS = ['P_TOT', 'Q_TOT', 'S_TOT', 'PF_TOT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_to_pandas(hdf_dataset):\n",
    "    column_type_dict = {x:str(y[0]) for x,y in hdf_dataset.dtype.fields.items()}\n",
    "    column_list = []\n",
    "    for index in column_type_dict:\n",
    "        column_list.append(index)\n",
    "    list_of_rows = []\n",
    "    for line in range(0, hdf_dataset.size):\n",
    "        list_of_rows.append(np.asarray(hdf_dataset[line]).tolist())\n",
    "    return pd.DataFrame(data=list_of_rows, columns=column_list)\n",
    "\n",
    "def first_n_digits(num, n):\n",
    "    return num // 10 ** (int(math.log(num, 10)) - n + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in hdf5 data and convert to pandas format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2018\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2018_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2018 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2018[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2019\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2019_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2019 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2019[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# 2020\n",
    "file = h5py.File('Data/HDF5data/heatpumps/2020_data_15min.hdf5', 'r')\n",
    "dset_no_pv = file['NO_PV']\n",
    "dset_pv = file[\"WITH_PV\"]\n",
    "\n",
    "df_dict_2020 = {}\n",
    "for key in dset_no_pv.keys():\n",
    "    #dset_house = dset_no_pv[key]\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_no_pv[key][\"HEATPUMP\"]['table'])\n",
    "for key in dset_pv.keys():\n",
    "    df_dict_2020[key] = hdf_to_pandas(dset_pv[key][\"HEATPUMP\"]['table'])\n",
    "\n",
    "# concat \n",
    "df_dict = {}\n",
    "\n",
    "for key_house in df_dict_2020:\n",
    "    df_dict[key_house] = pd.concat([df_dict_2018[key_house], df_dict_2019[key_house], df_dict_2020[key_house]])\n",
    "\n",
    "for key_house in df_dict:\n",
    "    if len(df_dict[key_house]) != 105216:\n",
    "        print(\"issue with \" + str(key_house))\n",
    "\n",
    "print(\"data for {} houses\".format(len(df_dict)))\n",
    "\n",
    "with open('Data/heatpump/data_heatpump.pkl', 'wb') as f:\n",
    "    pickle.dump(df_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dataframe, which indicates missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "with open('Data/heatpump/data_heatpump.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(x):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_nan)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_availability_histogramm(df):\n",
    "    # Datenverfügbarkeit berechnen\n",
    "    data_availability = df.mean(axis=0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Farben festlegen\n",
    "    colors = ['#ff9999' if value < 1 else '#66b266' for value in data_availability]\n",
    "\n",
    "    # Horizontales Balkendiagramm zeichnen\n",
    "    data_availability.plot(kind='barh', color=colors, ax=ax)\n",
    "\n",
    "    # Achsentitel und Plot-Titel hinzufügen\n",
    "    ax.set_title('Datenverfügbarkeit')\n",
    "    ax.set_ylabel('Objekte')\n",
    "    ax.set_xlabel('Verfügbarkeit in %')\n",
    "    ax.set_xlim(0, 1)  # x-Achse auf den Bereich 0 bis 1 setzen\n",
    "\n",
    "    # Anzeigen\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_data_availability(data):\n",
    "    df = data.copy()\n",
    "    # Datenkonvertierung: Unix-Timestamp zu Datum\n",
    "    df.index = pd.to_datetime(df.index, unit='s')\n",
    "    print(df.index[0])\n",
    "\n",
    "    # Reihenfolge der Spalten nach den Zahlen in den Objektbezeichnungen sortieren\n",
    "    sorted_columns = sorted(df.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "\n",
    "    # Prozentsätze für jede Spalte berechnen\n",
    "    percentages = (df.sum() / len(df) * 100).round(2)\n",
    "    percentages = percentages[sorted_columns]\n",
    "\n",
    "    # Plot-Einstellungen, Verkleinerung der Figur\n",
    "    fig, ax = plt.subplots(figsize=(7, 8))  # Kleinere Figurgröße\n",
    "\n",
    "    # Durch jede sortierte Spalte iterieren und Datenverfügbarkeit zeichnen\n",
    "    for i, column in enumerate(sorted_columns):\n",
    "        # Datenverfügbarkeit\n",
    "        ax.fill_between(df.index, i, i + 1, where=(df[column] == 1), color='#66D37A', step='mid')\n",
    "        # Fehlende Daten\n",
    "        ax.fill_between(df.index, i, i + 1, where=(df[column] == 0), color='#FF5252', step='mid')\n",
    "        # Prozentsatz neben jedem Balken hinzufügen (mit zusätzlichem Leerzeichen und Abstand nach rechts)\n",
    "        ax.text(df.index[-1] + pd.Timedelta(days=1), i + 0.5, f\" {percentages[column]}%\", verticalalignment='center', horizontalalignment='left')\n",
    "\n",
    "    # Einstellungen\n",
    "    # Anpassung der x-Limits, um den Platz für die Prozentsätze zu berücksichtigen\n",
    "    ax.set_xlim([df.index.min(), df.index.max() + pd.Timedelta(days=1)])  # Reduzierter Platz für Prozentsätze\n",
    "    ax.set_ylim([0, len(sorted_columns)])\n",
    "    ax.set_yticks(np.arange(len(sorted_columns)) + 0.5)\n",
    "    ax.set_yticklabels(sorted_columns)\n",
    "    ax.set_title(\"Data Availability\")\n",
    "    ax.set_xlabel(\"Timestamps\")\n",
    "    ax.set_ylabel(\"Objects\")\n",
    "\n",
    "    # Farblegende hinzufügen\n",
    "    ax.legend(handles=[plt.Line2D([0], [0], color='#66D37A', label='Available'),\n",
    "                    plt.Line2D([0], [0], color='#FF5252', label='Missing')], loc='upper right')\n",
    "\n",
    "    # Erweiterte Datumseinstellungen für die X-Achse mit 3-monatigem Intervall\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability_histogramm(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop functions to analize intervalls with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_intervalls(df, column):\n",
    "    # Gruppen von zusammenhängenden Nullen identifizieren\n",
    "    df['group'] = (df[column] != df[column].shift()).cumsum()\n",
    "    zero_groups = df[df[column] == 0].groupby('group')\n",
    "    # Start- und Endindizes von zusammenhängenden Nullen ausgeben\n",
    "    result = []\n",
    "    for name, group in zero_groups:\n",
    "        start_index = group.index[0]\n",
    "        end_index = group.index[-1]\n",
    "        result.append((start_index, end_index))\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_missing_intervalls_length(tuple):\n",
    "    length = tuple[1] - tuple[0]\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10 = df_result['SFH10'].to_frame()\n",
    "df_10.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_intervalls(df_10, 'SFH10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce dataframe to common start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_result[df_result.index >= 1525270500]\n",
    "\n",
    "for row,value in test.iterrows():\n",
    "    if test.loc[row].sum() > 30:\n",
    "        print(\">30: \" + str(row))\n",
    "        start_index = row\n",
    "        break\n",
    "\n",
    "df_final = df_result[df_result.index >= start_index]\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability_histogramm(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_availability(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = df_result[df_result.index >= 1528965000] # index from start of data availability of SFH37\n",
    "plot_data_availability(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.85\n",
    "column_list=[]\n",
    "missing_list=[]\n",
    "incomplete_list = []\n",
    "complete_list = []\n",
    "for column in df_reduced.columns:\n",
    "    percentage = df_reduced[column].sum()/len(df_reduced)\n",
    "    if percentage > threshold:\n",
    "        if percentage != 1:\n",
    "            incomplete_list.append(column)\n",
    "        if percentage ==1: \n",
    "            complete_list.append(column)\n",
    "        column_list.append(column)\n",
    "    else:\n",
    "        missing_list.append(column)\n",
    "\n",
    "print('reduced to {} datasets'.format(len(column_list)))\n",
    "\n",
    "plot_data_availability(df_reduced[column_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Analysis of missing intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_intervalls = {}\n",
    "\n",
    "df_result = df_result[df_result.index > INDEX_START_2]\n",
    "for column in df_result.columns:\n",
    "    if column in incomplete_list:\n",
    "        df = df_result[column].to_frame()\n",
    "        intervalls = get_missing_intervalls(df, column)\n",
    "        dict_intervalls[column] = intervalls\n",
    "        #print(column + \": \" + str(intervalls))\n",
    "\n",
    "#with open('Data/missing_intervalls_dict.pkl', 'wb') as f:\n",
    "#    pickle.dump(dict_intervalls, f)\n",
    "\n",
    "dict_intervalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"complete list 100%: \" + str(complete_list))\n",
    "print(\"incomplete list >85%: \" + str(incomplete_list))\n",
    "print(\"insufficient list <85%: \" + str(missing_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of all available data for whole timespan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumption_plot(data, name):\n",
    "    df = data.reset_index()\n",
    "    # Erstelle eine Figur mit sekundärer Y-Achse\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Kurven für P_TOT, Q_TOT, und S_TOT zur linken Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['P_TOT'], name='P_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['Q_TOT'], name='Q_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['S_TOT'], name='S_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    # Füge die Kurve für PF_TOT zur rechten Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df['index'], y=df['PF_TOT'], name='PF_TOT', line=dict(dash='dot')),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    # Benenne die Achsen\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(\n",
    "        title_text='Zeitliche Darstellung der Werte - {}'.format(name),\n",
    "        xaxis=dict(\n",
    "            tickmode='auto',\n",
    "            nticks=20,\n",
    "            ticks='outside',\n",
    "            tickson='boundaries',\n",
    "            ticklen=20\n",
    "        )\n",
    "    )\n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "#create_consumption_plot(df_3, 'SFH3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampled version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resampled_consumption_plot(data, name):\n",
    "    # Stelle sicher, dass 'index' in datetime umgewandelt wird und als Index gesetzt wird\n",
    "    df = data.copy()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "    df.set_index('index', inplace=True)\n",
    "\n",
    "    # Resample der Daten auf 24-Stunden-Intervalle und berechne den Durchschnitt\n",
    "    df_resampled = df.resample('24H').mean().reset_index()\n",
    "\n",
    "    # Erstelle eine Figur mit sekundärer Y-Achse\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Kurven für P_TOT, Q_TOT, und S_TOT zur linken Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['P_TOT'], name='P_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['Q_TOT'], name='Q_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['S_TOT'], name='S_TOT'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    # Füge die Kurve für PF_TOT zur rechten Y-Achse hinzu\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df_resampled['index'], y=df_resampled['PF_TOT'], name='PF_TOT', line=dict(dash='dot')),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "\n",
    "    # Benenne die Achsen\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(\n",
    "        title_text='Zeitliche Darstellung der Werte mit 24-Stunden-Intervallen - {}'.format(name),\n",
    "        xaxis=dict(\n",
    "            tickmode='auto',\n",
    "            nticks=20,\n",
    "            ticks='outside',\n",
    "            tickson='boundaries',\n",
    "            ticklen=20\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Zeige die Figur an\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_key(string):\n",
    "    # Finde alle Zahlen im String und verbinde sie\n",
    "    number = int(re.search(r'\\d+', string).group())\n",
    "    return number\n",
    "\n",
    "list_strings = list(load_dict.keys())\n",
    "sorted_keys = sorted(list_strings, key=sort_key)\n",
    "\n",
    "for key in sorted_keys[2:4]: #remove [0:5] to see all\n",
    "    sub_df = load_dict[key]\n",
    "    sub_df = sub_df[sub_df['index']>INDEX_START]\n",
    "    sub_df.set_index('index', inplace=True)\n",
    "    sub_df.index = pd.to_datetime(sub_df.index, unit='s')\n",
    "    sub_df.reset_index(inplace=True)\n",
    "    create_resampled_consumption_plot(sub_df, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_year(df_year, year, name):\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year['index'], y=df_year['PF_TOT'], name='PF_TOT', line=dict(dash='dot')), secondary_y=True)\n",
    "    \n",
    "    # Update the layout for the subplot\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "    fig.update_xaxes(title_text='Zeit')\n",
    "\n",
    "    # Füge einen Titel hinzu und passe das Layout an\n",
    "    fig.update_layout(height=600, width=1200, title_text=f'Daten für das Jahr {year} - {name}')\n",
    "    \n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "def create_plot_per_year(data, name, years=[2018,2019,2020]):\n",
    "    df = data.copy()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "\n",
    "    # Filtere die Daten nach Jahr\n",
    "    for year in years:\n",
    "        df_year = df[df['index'].dt.year == year]\n",
    "        plot_year(df_year, year, name)\n",
    "\n",
    "#create_plot_per_year(df_3, 'SFH3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erstellung der Plots mit 24-stündiger Auflösung\n",
    "def create_resampled_plot_per_year(data, year, name):\n",
    "    df = data.reset_index()\n",
    "    df['index'] = pd.to_datetime(df['index'])\n",
    "    df.set_index('index', inplace=True)\n",
    "    # Resample der Daten auf 24-Stunden-Intervalle und berechne den Durchschnitt\n",
    "    df_resampled = df.resample('24H').mean()\n",
    "    \n",
    "    # Filtere nur das gewünschte Jahr\n",
    "    df_year = df_resampled[df_resampled.index.year == year]\n",
    "\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Füge die Daten zur Figur hinzu\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df_year.index, y=df_year['PF_TOT'], name='PF_TOT', line=dict(dash='dot')), secondary_y=True)\n",
    "\n",
    "    # Aktualisiere die Layout-Einstellungen\n",
    "    fig.update_layout(title_text=f\"Durchschnittliche Werte in 24-Stunden-Intervallen für das Jahr {year} - {name}\",\n",
    "                      height=600, width=1400)\n",
    "    fig.update_yaxes(title_text='P_TOT, Q_TOT, S_TOT', secondary_y=False)\n",
    "    fig.update_yaxes(title_text='PF_TOT', secondary_y=True)\n",
    "\n",
    "    # Zeige die Figur an\n",
    "    fig.show()\n",
    "\n",
    "# Erzeuge die Plots für jedes Jahr\n",
    "# for year in range(2018,2021,1):\n",
    "    #create_resampled_plot_per_year(df_3, year, 'SFH3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_moving_average(data, bez):\n",
    "    # Angenommen, df ist Ihr DataFrame, der bereits geladen wurde und das Datum als Index hat.\n",
    "    df=data.resample('24H').mean()\n",
    "\n",
    "    # Erstellen Sie eine Subplot-Figur mit zwei Y-Achsen\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Hinzufügen der Linien für 'P_TOT', 'Q_TOT', und 'S_TOT' auf der linken Y-Achse\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['P_TOT'], name='P_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['Q_TOT'], name='Q_TOT'), secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['S_TOT'], name='S_TOT'), secondary_y=False)\n",
    "\n",
    "    # Hinzufügen der Linie für 'PF_TOT' auf der rechten Y-Achse\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['PF_TOT'], name='PF_TOT', marker_color='red'), secondary_y=True)\n",
    "\n",
    "    # Berechnen des gleitenden Mittelwerts für 'P_TOT' und Hinzufügen als Linie\n",
    "    df['P_TOT_SMA'] = df['P_TOT'].rolling(window=3).mean()\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=df['P_TOT_SMA'], name='P_TOT SMA', line=dict(color='firebrick', width=2, dash='dash')), secondary_y=False)\n",
    "\n",
    "    # Update der Layouts für Achsen und Titel\n",
    "    fig.update_layout(title_text=\"24h resampled Lastverlauf - {}\".format(bez), title_x=0.5)\n",
    "    fig.update_xaxes(title_text=\"\", dtick='M1')\n",
    "    fig.update_yaxes(title_text=\"P_TOT, Q_TOT, S_TOT [W, var, VA]\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"PF_TOT\", secondary_y=True)\n",
    "\n",
    "    # Anzeigen der Figur\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Aggregated load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = load_dict[\"SFH3\"]\n",
    "df_3 = df_3[df_3['index']>INDEX_START]\n",
    "df_3.set_index('index', inplace=True)\n",
    "df_3.index = pd.to_datetime(df_3.index, unit='s')\n",
    "\n",
    "df_summe = pd.DataFrame(index=df_3.index, columns=COLUMNS)\n",
    "for key in load_dict:\n",
    "    df_house = load_dict[key].copy()\n",
    "    df_house['index'] = pd.to_datetime(df_house['index'], unit='s')\n",
    "    df_house.set_index('index', inplace=True)\n",
    "    df_house = df_house[df_house.index > pd.to_datetime(start_index, unit='s')]\n",
    "    df_house = df_house[COLUMNS]\n",
    "    df_summe = df_summe.fillna(0) + df_house.fillna(0)\n",
    "# normieren des Leistungsfaktor    \n",
    "df_summe['PF_TOT'] = df_summe['PF_TOT']/len(load_dict)\n",
    "plot_with_moving_average(df_summe, 'Summe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Influence of day type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summe.index = pd.to_datetime(df_summe.index, unit='s')\n",
    "\n",
    "df_wochentage = df_summe[df_summe.index.dayofweek < 5]\n",
    "df_wochenende = df_summe[df_summe.index.dayofweek >= 5]\n",
    "\n",
    "weekdays_avg = df_wochentage[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "weekend_avg = df_wochenende[['P_TOT', 'Q_TOT', 'S_TOT']].mean()\n",
    "\n",
    "# Erstellen eines neuen DataFrames mit den Durchschnittswerten\n",
    "df_avg = pd.DataFrame({'Wochentage': weekdays_avg, 'Wochenende': weekend_avg})\n",
    "\n",
    "# Erstellen eines horizontalen Balkendiagramms\n",
    "fig = go.Figure()\n",
    "\n",
    "# Hinzufügen der Balken für jeden Datensatz\n",
    "for i, col in enumerate(df_avg.columns):\n",
    "    fig.add_trace(go.Bar(\n",
    "        y=df_avg.index,  # Spaltennamen werden auf der y-Achse angezeigt\n",
    "        x=df_avg[col],  # Durchschnittswerte werden auf der x-Achse angezeigt\n",
    "        name=col,  # Legendenname\n",
    "        orientation='h',  # Horizontale Balken\n",
    "        text=df_avg[col].round(2),\n",
    "        textposition='inside'\n",
    "       # marker_color=colors[i]  # Farbe der Balken\n",
    "    ))\n",
    "\n",
    "# Aktualisieren des Layouts für ein gruppiertes Balkendiagramm\n",
    "fig.update_layout(\n",
    "    barmode='group',  # Gruppierung der Balken\n",
    "    title='Durchschnittswerte von Wochentagen und Wochenenden',\n",
    "    title_x = 0.5,\n",
    "    xaxis_title='Durchschnittswerte',\n",
    "    yaxis_title='',\n",
    "    legend_title='Tagtyp',\n",
    "    bargap=0.2,  # Abstand zwischen den Balkengruppen\n",
    "    template='simple_white'\n",
    ")\n",
    "\n",
    "# Anzeigen des Diagramms\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Operation modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > P < 100W: Standby\n",
    "- > 100W < P < 4kW: compressor mode\n",
    "- > P > 4kW: heating rod mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_consumtion_type_histo(df_consumptions, years):\n",
    "    df_consumptions.reset_index(inplace=True)\n",
    "    # Erstellen Sie das Balkendiagramm\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Hinzufügen der Balken für jede Kategorie\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Standby'],\n",
    "        name='Stand-by-Modus',\n",
    "        marker_color='green'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Kompressions-Modus'],\n",
    "        name='Kompressions-Modus',\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df_consumptions['index'],\n",
    "        y=df_consumptions['Heizstab-Modus'],\n",
    "        name='Heizstab-Modus',\n",
    "        marker_color='red'\n",
    "    ))\n",
    "\n",
    "    # Update das Layout\n",
    "    fig.update_layout(\n",
    "        title='Verbrauchte Wirkleistung in kWh/a - {}'.format(years),\n",
    "        title_x = 0.5,\n",
    "        xaxis_tickangle=-45,\n",
    "        xaxis_title='Haushalt',\n",
    "        yaxis_title='Wirkleistung in kWh/a',\n",
    "        barmode='group',\n",
    "        legend_title='Legend',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Zeigen Sie die Figur an\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "#only for columns as index \n",
    "def check_operation_mode(x):\n",
    "    if x < 100:\n",
    "        return 1\n",
    "    elif (x > 100) & (x < 4000):\n",
    "        return 2\n",
    "    elif x >=4000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df_result = load_dict['SFH10']['index'].to_frame()\n",
    "for df in load_dict:\n",
    "    load_dict[df][df] = load_dict[df]['P_TOT'].apply(check_operation_mode)\n",
    "    df_result = pd.concat([df_result, load_dict[df][df]], axis=1)\n",
    "df_result.set_index('index', inplace=True)\n",
    "##########\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in sorted_columns:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions.columns:\n",
    "    df_consumptions[column] = df_consumptions[column]/1000\n",
    "df_consumptions.head()\n",
    "plot_consumtion_type_histo(df_consumptions, '2018-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2018 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2018.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2018]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2018.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2018.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2018.columns:\n",
    "    df_consumptions_2018[column] = df_consumptions_2018[column]/1000\n",
    "df_consumptions_2018.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2018, 2018)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2019 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2019.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2019]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2019.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2019.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2019.columns:\n",
    "    df_consumptions_2019[column] = df_consumptions_2019[column]/1000\n",
    "df_consumptions_2019.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2019,2019)\n",
    "\n",
    "sorted_columns = sorted(df_result.columns, key=lambda x: int(x.replace(\"SFH\", \"\")))\n",
    "df_consumptions_2020 = pd.DataFrame(index=sorted_columns, columns=['Standby', 'Kompressions-Modus', 'Heizstab-Modus'])\n",
    "\n",
    "for index in df_consumptions_2020.index:\n",
    "    df_house = load_dict[index].set_index('index')['P_TOT'].to_frame().fillna(0)\n",
    "    df_house.index = pd.to_datetime(df_house.index, unit='s')\n",
    "    df_house = df_house[df_house.index.year==2020]\n",
    "    df_house = df_house.resample('H').mean()\n",
    "    df_consumptions_2020.loc[index]['Standby'] = df_house[df_house['P_TOT']<100]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Kompressions-Modus'] = df_house[(df_house['P_TOT']>100)&(df_house['P_TOT']<4000)]['P_TOT'].sum()\n",
    "    df_consumptions_2020.loc[index]['Heizstab-Modus'] = df_house[df_house['P_TOT']>=4000]['P_TOT'].sum()\n",
    "\n",
    "for column in df_consumptions_2020.columns:\n",
    "    df_consumptions_2020[column] = df_consumptions_2020[column]/1000\n",
    "df_consumptions_2020.head()\n",
    "plot_consumtion_type_histo(df_consumptions_2020, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Additional project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_excel('Data/Gebaeudeinformationen.xlsx', header=0)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker = False\n",
    "for index in df_dict_2018:\n",
    "    if (len(df_dict_2018[index]['index'])) != 35040:\n",
    "        print(\"issue with index \" + str(index))\n",
    "        checker = True\n",
    "if not checker:\n",
    "    print('all indices have the same size (1,35040)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for house in df_dict:\n",
    "    df_dict[house]['time_difference'] = df_dict[house]['index'] - df_dict[house]['index'].shift(1)\n",
    "    if df_dict[house]['time_difference'].value_counts()[900.0] != 105215:\n",
    "        print(house)\n",
    "else:\n",
    "    print('time stamps continues 15 min intervalls')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
