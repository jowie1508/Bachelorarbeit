{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from datetime import datetime\n",
    "import nbformat\n",
    "\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import random\n",
    "random.seed(2505)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data/heatpump/data_heatpump_cleaned_v1.pkl', 'rb') as f:\n",
    "    load_dict = pickle.load(f)\n",
    "\n",
    "with open('Data/weather/data_weather_v1.pkl', 'rb') as f:\n",
    "    weather_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging load and weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_info = pd.read_excel(\"Data/Gebaeudeinformationen_cleaned.xlsx\", index_col=0)\n",
    "building_info.set_index(\"Building number\", inplace=True)\n",
    "\n",
    "# add building information\n",
    "for house in load_dict:\n",
    "    id = int(re.findall(r'\\d+', house)[0])\n",
    "\n",
    "    load_dict[house][\"area\"] = building_info.loc[id][\"Building area\"]\n",
    "    load_dict[house][\"inhabitants\"] = building_info.loc[id][\"Number of inhabitants\"]\n",
    "    load_dict[house][\"building\"] = id\n",
    "    \n",
    "    weather_data = weather_data[weather_data.index>=1528965900]\n",
    "    load_dict[house] = pd.concat([load_dict[house], weather_data], axis=1)\n",
    "    \n",
    "# concat consumption and weather data\n",
    "df = pd.concat(load_dict)\n",
    "df = df.reset_index().set_index('index').sort_index().drop(columns=[\"level_0\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"building\"]==5]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainieren des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "from numpy import newaxis\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datennormierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Erstellung von Sequenzen\n",
    "## Variante 1: getrennte Zielvariablen\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    outputs_p = []\n",
    "    outputs_pf = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        outputs_p.append(data.iloc[i+sequence_length]['P_TOT'])\n",
    "        outputs_pf.append(data.iloc[i+sequence_length]['PF_TOT'])\n",
    "\n",
    "    return np.array(sequences), np.array(outputs_p), np.array(outputs_pf)\n",
    "\n",
    "#sequence_length = 7  # Beispiel für eine Woche\n",
    "#X, y_p, y_pf = create_sequences(df_scaled, sequence_length)\n",
    "\n",
    "## Variante 2: gemeinsame Zielvariable\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    outputs = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        outputs.append(data.iloc[i+sequence_length][['P_TOT', 'PF_TOT']].values)\n",
    "\n",
    "    return np.array(sequences), np.array(outputs)\n",
    "\n",
    "sequence_length = 7\n",
    "sequence_length = sequence_length * 4 * 24\n",
    "X, y = create_sequences(df_scaled, sequence_length)\n",
    "\n",
    "# Überprüfen der Dimensionen\n",
    "print(\"Shape von X:\", X.shape)\n",
    "print(\"Shape von y:\", y.shape)\n",
    "\n",
    "# Visualisieren einiger Sequenzen\n",
    "for i in range(3):  # Die ersten 3 Sequenzen\n",
    "    print(f\"Sequenz {i}:\")\n",
    "    print(X[i])\n",
    "    print(f\"Zugehörige Zielwerte: {y[i]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Überprüfen der letzten Sequenz\n",
    "print(\"Letzte Sequenz und zugehöriger Zielwert:\")\n",
    "print(X[-1])\n",
    "print(y[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modell-Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Eingabeschicht\n",
    "model.add(GRU(units=50,                 # Hyperparameter -> kann variiert und angepasst werden\n",
    "              return_sequences=False,   # Konfigurationsparameter, default: False\n",
    "                                        # Funktionalität:   bestimmt, ob die Schicht einen Ausgabevektor für jeden Zeitpunkt in der Eingabesequenz (return_sequences=True) \n",
    "                                        #                   oder nur für den letzten Zeitpunkt (return_sequences=False) zurückgeben soll\n",
    "                                        #   False: gibt Ausgabevektor für den letzten Zeitschritt zurück: (Anzahl der Beispiele, Anzahl der Units)\n",
    "                                        #   True:  gibt Ausgabevektor für jeden Zeitschritt in der Eingabesequenz zurück: (Anzahl der Beispiele, Anzahl der Zeitschritte, Anzahl der Units)\n",
    "                                        # Anwendung: return_sequences=True: mehrere rekurrente Schichten hintereinander (damit jede Schicht eine Sequenz an die nächste weitergibt),  Ausgabe des Modells ist selbst eine Sequenz; \n",
    "                                        #            ansonsten: return_sequences=False.\n",
    "                                        # \n",
    "              input_shape=(\n",
    "                  X.shape[1],           # Sequenzlänge\n",
    "                  X.shape[2]            # Anzahl der Features\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "# Dropout-Schicht\n",
    "model.add(Dropout(0.2))  # Optional: Dropout zur Vermeidung von Overfitting durch zufälliges Deaktivieren von Neuronen während des Trainingsprozesses -> Vermeidung dominanter Neuronen -> bessere Generalisierung\n",
    "\n",
    "if hidden_layers:\n",
    "    # weitere GRU-Schichten\n",
    "    # optional\n",
    "    model.add(GRU(50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Hinzufügen der letzten GRU-Schicht ohne return_sequences (optional, bei der Verwendung mehrerer GRU-Schichten notwendig)\n",
    "    model.add(GRU(50))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Hinzufügen einer Dense-Schicht zur weiteren Merkmalsextraktion (optional)\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "\n",
    "# Ausgabeschicht\n",
    "model.add(Dense(2))  # Zwei Units für die zwei Zielvariablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisierung der Modellstruktur\n",
    "model.summary()\n",
    "# plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kompilieren des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurieren des Modells für das Training -> Festlegung der Lernart sowie die Bewertung des Trainingsprozesses\n",
    "model.compile(\n",
    "    optimizer='adam',           # der Optimizer ist ein Algorithmus zur Aktualisierung des Netzwerks, wobei die Gewichte des Modells so angepasst werden, dass Verluste minimiert werden\n",
    "                                # Verschiedene Optimierer haben unterschiedliche Eigenschaften:\n",
    "                                # Adam - adaptive moment estimation: Grundprinzipien\n",
    "                                #   - Adaptive Lernraten: Lernrate wird für jeden Parameter individuell angepasst, basierend auf der Schätzung des ersten Mittelwert und des zweiten Moments der Gradienten\n",
    "                                #   - Moment-Schätzungen:   > erstes Moment (Mittelwert): Berechnung expontentiell abnehmender Durchschnittswerte vergangener Gradienten -> Steuerung zu relevanten Richtung des Gradientenabstiegs\n",
    "                                #                           > zweites Moment(Varianz): Berechnung exponentiuell abnehmender Durchschnittswerte vergangener quadrierter Gradienten \n",
    "                                #                                                       -> Adaption der Lernrate, Regulierung der Schrittgröße basierend auf der Unsicherheit des Gradienten\n",
    "                                #   - Korrektur der Bias: Verhinderung der Tendenz, das Schätzungen zu Beginn gegen 0 gehen\n",
    "                                # Vorteile: Effizienz, wenigeer manuelle Einstellung der Lernrate, gute Performance bei großen Datenmengen/vielen Parametern\n",
    "    loss='mean_squared_error',  # Verlustfunktion, misst die Genauigkeit des Modells. MSE misst die durchschnittliche quadratische Abweichung zwischen den vorhergesagten und den tatsächlichen Werten\n",
    "    metrics=['mean_absolute_error']      # Metriken, die für das Training bewertet werden sollen, weitere Alternativen: 'accuracy', ...\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainieren des Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilung in Trainings-, Test- und Validierungsdaten\n",
    "# Ansatz: Chronologische Aufteilung ohne Überlappung\n",
    "# sonst: sklearn.train_test_split() mit zufälliger Aufteilung der Daten -> Problem: Vorhersage der Vergangenheit mit Werten aus der Zukunft?\n",
    "\n",
    "train_size = int(len(X) * 0.7)\n",
    "val_size = int(len(X) * 0.85)\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:val_size], y[train_size:val_size]\n",
    "X_test, y_test = X[val_size:], y[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time_series_generator import TimeSeriesGenerator\n",
    "\n",
    "# Erstellen des Generators\n",
    "SEQUENCE_LENGTH = 7*4*24\n",
    "train_generator = TimeSeriesGenerator(X_train, y_train, length=SEQUENCE_LENGTH, batch_size=32)\n",
    "val_generator = TimeSeriesGenerator(X_val, y_val, length=SEQUENCE_LENGTH, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainieren des Modells\n",
    "history = model.fit(\n",
    "    X_train, y_train,                  # Übergabe der Trainingsdaten\n",
    "    #train_generator,\n",
    "    epochs=50,                          # Anzahl der Durchläufe des gesamten Trainingsdatensatzes\n",
    "                                        #   -> Einfluss: Mehr Epochen können zu einer besseren Anpassung des Modells führen <-> Gefahr des Overfittings\n",
    "    batch_size=64,                      # Bestimmt die Anzahl der verwendeten Datenpunkte für eine Iteration, bevor die Modellgewichte aktualisiert werden\n",
    "                                        #   -> größere Batch-Größen: stabilere, aber langsamer konvergierende Updates <-> kleinere Batch-Größen: schnellere, weniger stabile Updates\n",
    "    validation_data=(X_val, y_val),     # Validierungsdaten, ermöglichen die Überwachung des Trainingsprozesses -> Erkennung von Overfitting\n",
    "    #validation_data=val_generator,\n",
    "    use_multiprocessing=True,           # Laufzeitoptimierung\n",
    "    workers=6,                          # Nutzen mehrerer CPU-Kerne\n",
    "    verbose=1,                          # Steuert die Menge an Infos, welche während des Trainings ausgegeben werden -> verbose=1 zeigt den Fortschritt für jede Epoche an\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speichern des trainierten Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/GRU_1layer_30_nodes.h5')  # Speichert das Modell im HDF5-Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('models/GRU_1layer_30_nodes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Wie speicher ich die Ergebnisse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisierung der Trainingsverlaufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_functions import plot_validation_loss\n",
    "from test import plot_pred_vs_act_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_validation_loss(history, \"- Model 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluierung des Modells mit den Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print('Testverlust:', test_loss)\n",
    "# Vorhersagen auf den Testdaten machen\n",
    "predicted_test = model.predict(X_test)\n",
    "print(\"Shape predicted_test {}\".format(predicted_test.shape))\n",
    "print(\"Shape y_test {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rückskalierung der Daten\n",
    "# Da der scaler auf alle Trainingsdaten angewendet worden ist, müssen die in den Vorhersagedaten fehlenden Spalten (Wetterdaten und Gebäudedaten) aufgefüllt werden\n",
    "predicted_test_inversed = scaler.inverse_transform(\n",
    "    np.hstack((predicted_test, np.zeros((predicted_test.shape[0], df.shape[1] - 2))))\n",
    ")[:, :2]\n",
    "\n",
    "y_test_inversed = scaler.inverse_transform(\n",
    "    np.hstack((y_test, np.zeros((y_test.shape[0], df.shape[1] - 2))))\n",
    ")[:, :2]\n",
    "\n",
    "# Zeitstempel\n",
    "timestamps = df.index\n",
    "test_timestamps = timestamps[val_size + sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_act_cons(y_test_inversed, predicted_test_inversed, test_timestamps, \"2020-12-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test import print_metrics\n",
    "print_metrics(y_test_inversed, predicted_test_inversed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
